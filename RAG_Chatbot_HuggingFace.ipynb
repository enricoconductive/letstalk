{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot - HuggingFace Version (Hong Kong Compatible)\n",
    "\n",
    "## What This Does\n",
    "\n",
    "This notebook creates a chatbot that answers questions about your PDF documents using **HuggingFace Inference API** (works in Hong Kong without VPN).\n",
    "\n",
    "**You Need:**\n",
    "- PDF files in your Google Drive\n",
    "- A free HuggingFace API token (~300 requests/hour)\n",
    "- Internet connection\n",
    "\n",
    "**Cost:** 100% FREE\n",
    "\n",
    "**Why HuggingFace?** Google Gemini requires VPN in Hong Kong/China. HuggingFace works without VPN and offers access to many open-source models.\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** See STUDENT_GUIDE.md for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Libraries\n",
    "\n",
    "This installs the required tools. Takes about 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q chromadb gradio pypdf sentence-transformers huggingface_hub vaderSentiment\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"\\n‚ÑπÔ∏è  Note: You may see dependency warnings about 'opentelemetry' packages.\")\n",
    "print(\"   These are non-critical and won't affect functionality. You can safely ignore them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load Libraries\n",
    "\n",
    "Load the tools we just installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from google.colab import drive\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Connect Google Drive\n",
    "\n",
    "**Steps:**\n",
    "1. Click the link that appears\n",
    "2. Choose your Google account\n",
    "3. Click \"Allow\"\n",
    "\n",
    "Your files will be at: `/content/drive/MyDrive/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(\"üìÅ Your files are available at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Configuration - CHANGE THESE! ‚úèÔ∏è\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: You must edit the values below**\n",
    "\n",
    "### Get Your FREE HuggingFace Token:\n",
    "1. Go to: https://huggingface.co/\n",
    "2. Sign up for free account (email + password)\n",
    "3. Go to: https://huggingface.co/settings/tokens\n",
    "4. Click \"New token\"\n",
    "5. Name it: \"Empathy Chatbot\"\n",
    "6. Type: \"Read\"\n",
    "7. Click \"Generate\"\n",
    "8. Copy the token (starts with `hf_`)\n",
    "9. Paste it in the next cell where it says `YOUR_API_TOKEN_HERE`\n",
    "\n",
    "**Free Tier:** ~300 requests per hour (plenty for this course!)\n",
    "\n",
    "### Add Your PDF Files:\n",
    "- Upload PDFs to your Google Drive\n",
    "- Update the `PDF_PATHS` list with your file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# 1. API TOKEN - ADD YOUR TOKEN HERE! ‚úèÔ∏è\n# ============================================================================\n# Get your FREE token from: https://huggingface.co/settings/tokens\n# Create a \"Fine-grained\" token with \"Make calls to Inference Providers\" enabled\n\nHUGGINGFACE_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # ‚úèÔ∏è REPLACE THIS with your token (starts with hf_)\n\n# ============================================================================\n# 2. MODEL SELECTION - RECOMMENDED: Meta-Llama-3.1-8B ‚úèÔ∏è\n# ============================================================================\n\nMODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # ‚úÖ Works with chat_completion, high quality\n\n# Alternative models (uncomment to try):\n# MODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"  # Fastest, most reliable\n# MODEL_NAME = \"google/gemma-2-2b-it\"  # Lightweight, instruction-tuned\n# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Good quality (use text_generation instead)\n\n# ============================================================================\n# 3. PERSONA - CUSTOMIZE THIS! ‚úèÔ∏è\n# ============================================================================\n\nPERSONA_NAME = \"Your Persona Name\"  # ‚úèÔ∏è CHANGE THIS - e.g., \"Albert Einstein\", \"Oprah Winfrey\"\n\n# ‚úèÔ∏è CUSTOMIZE THIS: Describe your persona's speaking style and personality\nPERSONA_DESCRIPTION = \"\"\"\nReplace this entire section with your persona's description.\n\nTemplate:\nYou are [NAME], [brief description/title/role].\nYou speak in a [adjective] manner, using [characteristic words/phrases].\nYou emphasize [what they care about] and often [communication patterns].\n\nInstructions for customization:\n- Describe HOW they speak (tone, word choice, sentence structure)\n- Include specific phrases or words they commonly use\n- Mention what topics/themes they emphasize\n- Note any unique speaking patterns or habits\n- Keep it focused on communication style, not just biographical facts\n\nExample:\n\"You are Marie Curie, pioneering scientist. You speak precisely and scientifically,\nusing terms like 'research,' 'experiment,' and 'discovery.' You emphasize evidence-based\nreasoning and the importance of persistence in scientific work.\"\n\"\"\"\n\n# Tip: Describe how your person speaks and thinks\n\n# ============================================================================\n# 4. RESPONSE SETTINGS - OPTIONAL\n# ============================================================================\n\nTEMPERATURE = 0.7  # Creativity level (0.0 = focused, 1.0 = creative)\n\nMAX_OUTPUT_TOKENS = 300  # Maximum response length\n\nNUM_RETRIEVED_DOCS = 7  # How many document pieces to search\n\n# ============================================================================\n# 5. CONVERSATION MEMORY - OPTIONAL\n# ============================================================================\n\nCONVERSATION_MEMORY = 3  # How many previous message pairs to remember (0 = no memory)\n# Higher values = more context but slower responses and higher token usage\n# Recommended: 3-5 for natural conversation\n\n# üí° EXAMPLES:\n#\n# CONVERSATION_MEMORY = 0  # No memory - each question treated independently\n#   ‚úÖ Best for: Quick Q&A, unrelated questions\n#   ‚úÖ Pros: Fast, low token usage\n#   ‚ùå Cons: Can't reference previous messages\n#\n# CONVERSATION_MEMORY = 3  # Remember last 3 exchanges (RECOMMENDED)\n#   ‚úÖ Best for: Natural conversation, follow-up questions\n#   ‚úÖ Pros: Contextual responses, feels more human\n#   ‚ùå Cons: Slightly slower\n#\n# CONVERSATION_MEMORY = 10  # Remember last 10 exchanges\n#   ‚úÖ Best for: Long, complex discussions\n#   ‚úÖ Pros: Deep context, can reference far back\n#   ‚ùå Cons: Slower, higher token usage, may hit limits\n\n# ============================================================================\n# 5B. DEBUG MODE - TESTING TOOL üß™\n# ============================================================================\n\nDEBUG_MEMORY = False  # Set to True to see conversation history sent to API\n# When enabled, you'll see:\n#   - How many messages are sent to the API\n#   - The exact content of each message (system/user/assistant)\n#   - Whether conversation memory is working correctly\n# Useful for testing and understanding how memory works!\n\n# ============================================================================\n# 6. SOURCE CITATIONS - OPTIONAL\n# ============================================================================\n\nSHOW_SOURCES = True  # True = show which PDFs were used, False = hide\n\n# ============================================================================\n# 7. CHUNKING SETTINGS - OPTIONAL\n# ============================================================================\n# How to split your PDFs into searchable pieces\n\nCHUNK_SIZE = 1000  # Characters per chunk (500-2000 recommended)\nOVERLAP = 200      # Overlap between chunks (keeps context)\n\n# üí° EXAMPLES - When to adjust:\n#\n# Example 1: SHORT & PRECISE (for quick facts)\n#   CHUNK_SIZE = 500\n#   OVERLAP = 100\n#   ‚úÖ Best for: Short Q&A, specific facts, definitions\n#   ‚úÖ Pros: Fast, precise answers\n#   ‚ùå Cons: May miss broader context\n#\n# Example 2: LONG & CONTEXTUAL (for complex topics)\n#   CHUNK_SIZE = 1500\n#   OVERLAP = 300\n#   ‚úÖ Best for: Detailed explanations, complex reasoning\n#   ‚úÖ Pros: Rich context, complete thoughts\n#   ‚ùå Cons: Slower, may include irrelevant info\n#\n# üéØ CURRENT (BALANCED): 1000 chars, 200 overlap\n#   ‚úÖ Works well for general conversation and empathy training\n\n# ============================================================================\n# 8. PDF FILES - ADD YOUR PDFS HERE! ‚úèÔ∏è\n# ============================================================================\n# Format: \"/content/drive/MyDrive/folder_name/file_name.pdf\"\n# Example: \"/content/drive/MyDrive/School/Materials/speech.pdf\"\n\nPDF_PATHS = [\n    \"/content/drive/MyDrive/your_folder/document1.pdf\",  # ‚úèÔ∏è REPLACE with your PDF path\n    \"/content/drive/MyDrive/your_folder/document2.pdf\",  # ‚úèÔ∏è Add more PDFs as needed\n    \"/content/drive/MyDrive/your_folder/document3.pdf\",\n    # Add more PDFs here...\n]\n\n# üí° TIP: To get the exact path:\n#   1. Click folder icon on left sidebar in Colab\n#   2. Navigate to your PDF file\n#   3. Right-click ‚Üí \"Copy path\"\n#   4. Paste here\n\n# ============================================================================\n# SETUP (Don't change this part)\n# ============================================================================\nclient = InferenceClient(token=HUGGINGFACE_TOKEN)\n\nprint(\"‚úÖ Configuration complete!\")\nprint(f\"üìã Persona: {PERSONA_NAME}\")\nprint(f\"ü§ñ Model: {MODEL_NAME}\")\nprint(f\"üåè API: HuggingFace (Hong Kong compatible)\")\nprint(f\"üìÑ PDF files: {len(PDF_PATHS)}\")\nprint(f\"üå°Ô∏è  Creativity: {TEMPERATURE}\")\nprint(f\"üìä Search pieces: {NUM_RETRIEVED_DOCS}\")\nprint(f\"üß† Conversation memory: {CONVERSATION_MEMORY} message pairs\")\nprint(f\"üß™ Debug mode: {'ON üîç' if DEBUG_MEMORY else 'OFF'}\")\nprint(f\"üìö Show sources: {'ON ‚úÖ' if SHOW_SOURCES else 'OFF'}\")\nprint(f\"üìè Chunk size: {CHUNK_SIZE} chars (overlap: {OVERLAP})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 5: Test API Connection\n",
    "\n",
    "**Run this first!** This checks if your API token works.\n",
    "\n",
    "‚úÖ If successful: Continue to next step  \n",
    "‚ùå If failed: Check your API token and try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üß™ Testing HuggingFace API connection...\")\nprint(\"=\" * 60)\n\ntry:\n    # Test using chat_completion API (better for conversational AI)\n    test_response = client.chat_completion(\n        messages=[\n            {\"role\": \"system\", \"content\": PERSONA_DESCRIPTION},\n            {\"role\": \"user\", \"content\": \"Say 'Hello! API is working!' in a friendly, enthusiastic style.\"}\n        ],\n        model=MODEL_NAME,\n        max_tokens=50,\n        temperature=0.7\n    )\n    \n    # Extract response from chat completion format\n    response_text = test_response.choices[0].message.content\n    \n    print(\"‚úÖ SUCCESS! HuggingFace API is working!\")\n    print(f\"\\nTest Response: {response_text}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚úÖ You can proceed with the rest of the notebook!\")\n    \nexcept Exception as e:\n    error_str = str(e).lower()\n    print(f\"‚ùå API TEST FAILED!\")\n    print(f\"Error: {str(e)}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚ö†Ô∏è  STOP! Fix this issue before proceeding:\")\n    \n    if \"503\" in str(e) or \"loading\" in error_str:\n        print(\"  üîÑ Model is loading for the first time (can take 20-30 seconds)\")\n        print(\"  üí° SOLUTION: Wait 30 seconds and run this cell again\")\n    elif \"invalid\" in error_str or \"token\" in error_str or \"401\" in str(e) or \"403\" in str(e):\n        print(\"  üîë Token issue detected\")\n        print(\"  üí° SOLUTION:\")\n        print(\"     1. Go to https://huggingface.co/settings/tokens\")\n        print(\"     2. Create 'Fine-grained' token\")\n        print(\"     3. Enable 'Make calls to Inference Providers' under Inference section\")\n        print(\"     4. Copy new token to Cell 8\")\n    elif \"model\" in error_str and (\"not found\" in error_str or \"does not exist\" in error_str):\n        print(\"  ü§ñ Model not available\")\n        print(\"  üí° SOLUTION: Try 'microsoft/Phi-3.5-mini-instruct' in Cell 8\")\n    else:\n        print(\"  1. Check your API token is correct (starts with hf_)\")\n        print(\"  2. Check your internet connection\")\n        print(\"  3. Visit https://huggingface.co/settings/tokens to verify your token\")\n        print(\"  4. Make sure token has 'Inference' permissions enabled\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## üß™ Step 5B: Test Conversation Memory (Optional)\n\n**What this does:** Tests if conversation memory is working correctly.\n\nThis cell simulates a multi-turn conversation and shows you:\n- How many messages are sent to the API\n- The exact format of the conversation history\n- Whether the bot can reference previous exchanges\n\n**When to run this:**\n- First time using the notebook (verify memory works)\n- After changing `CONVERSATION_MEMORY` setting\n- If responses don't seem to remember context\n\n**Instructions:**\n1. Make sure Cell 10 (API test) passed ‚úÖ\n2. Run this cell\n3. Check the output to see how memory is structured",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"üß™ TESTING CONVERSATION MEMORY\")\nprint(\"=\" * 80)\nprint(f\"\\nConfiguration:\")\nprint(f\"   ‚Ä¢ CONVERSATION_MEMORY = {CONVERSATION_MEMORY}\")\nprint(f\"   ‚Ä¢ Model: {MODEL_NAME}\")\nprint(f\"   ‚Ä¢ API: HuggingFace\")\n\n# Simulate a conversation history (what Gradio would provide)\nprint(f\"\\nüìù Simulating a 3-turn conversation...\\n\")\n\nsimulated_history = [\n    [\"What are your main beliefs?\", \"I believe in scientific progress and evidence.\"],\n    [\"Tell me more\", \"Science requires rigorous experimentation and peer review.\"],\n    [\"Why is that important?\", \"It prevents bias and ensures reproducible results.\"]\n]\n\nprint(\"Conversation so far:\")\nfor i, (user, bot) in enumerate(simulated_history, 1):\n    print(f\"   {i}. User: {user[:50]}...\")\n    print(f\"      Bot:  {bot[:50]}...\")\n\n# Show what would be sent to the API\nprint(f\"\\nüîç Building message array for API...\")\nprint(f\"   Memory setting: {CONVERSATION_MEMORY} exchanges\")\n\n# Simulate the message building logic from Cell 16\nPERSONA_TEST = \"You are a helpful assistant.\"\ncontext_test = \"Test context from documents.\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": f\"{PERSONA_TEST}\\n\\nContext: {context_test}\"\n    }\n]\n\n# Add conversation history (limited by CONVERSATION_MEMORY)\nif CONVERSATION_MEMORY > 0:\n    recent_history = simulated_history[-(CONVERSATION_MEMORY):]\n    print(f\"   Including last {len(recent_history)} exchanges from history\")\n    \n    for user_msg, bot_msg in recent_history:\n        messages.append({\"role\": \"user\", \"content\": user_msg})\n        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\nelse:\n    print(f\"   ‚ö†Ô∏è  Memory DISABLED - no history included\")\n\n# Add current question\ncurrent_question = \"What else can you tell me?\"\nmessages.append({\"role\": \"user\", \"content\": current_question})\n\n# Display message structure\nprint(f\"\\nüìä Message array to be sent to API:\")\nprint(f\"   Total messages: {len(messages)}\")\nprint(f\"   Breakdown:\")\nprint(f\"      - System message: 1\")\nprint(f\"      - History pairs: {(len(messages) - 2) // 2}\")\nprint(f\"      - Current question: 1\")\n\nprint(f\"\\nüìã Full message structure:\")\nprint(\"-\" * 80)\nfor i, msg in enumerate(messages):\n    role_emoji = \"üñ•Ô∏è\" if msg['role'] == \"system\" else (\"üë§\" if msg['role'] == \"user\" else \"ü§ñ\")\n    content_preview = msg['content'][:60].replace('\\n', ' ')\n    print(f\"   [{i}] {role_emoji} {msg['role']:10} | {content_preview}...\")\nprint(\"-\" * 80)\n\n# Verify memory is working as expected\nexpected_messages = 1 + (CONVERSATION_MEMORY * 2 if CONVERSATION_MEMORY > 0 else 0) + 1\nactual_messages = len(messages)\n\nprint(f\"\\n‚úÖ VERIFICATION:\")\nprint(f\"   Expected: {expected_messages} messages (1 system + {CONVERSATION_MEMORY*2 if CONVERSATION_MEMORY > 0 else 0} history + 1 current)\")\nprint(f\"   Actual:   {actual_messages} messages\")\n\nif actual_messages == expected_messages:\n    print(f\"   ‚úÖ PASS - Conversation memory structure is correct!\")\n    if CONVERSATION_MEMORY > 0:\n        print(f\"   ‚úÖ Memory is ENABLED - bot will remember last {CONVERSATION_MEMORY} exchanges\")\n        print(f\"   ‚úÖ Follow-up questions will work naturally\")\n    else:\n        print(f\"   ‚ö†Ô∏è  Memory is DISABLED - each question treated independently\")\nelse:\n    print(f\"   ‚ùå FAIL - Expected {expected_messages} but got {actual_messages}\")\n    print(f\"   üí° Check CONVERSATION_MEMORY setting in Cell 8\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"üí° To test with real API:\")\nprint(\"   1. Continue to Cell 12 (process PDFs)\")\nprint(\"   2. Run all cells through Cell 19 (launch chat)\")\nprint(\"   3. Set DEBUG_MEMORY = True in Cell 8 to see this output during chat\")\nprint(\"   4. Try asking follow-up questions like 'tell me more' or 'what else?'\")\nprint(f\"{'='*80}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Read PDF Files\n",
    "\n",
    "This reads your PDFs and splits them into small pieces for searching.\n",
    "\n",
    "**Time:** 1-2 minutes depending on file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Get text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n",
    "    \"\"\"Split text into small pieces (chunks) for better searching.\n",
    "    \n",
    "    Uses settings from Cell 8:\n",
    "    - chunk_size: Characters per chunk\n",
    "    - overlap: Characters that overlap between chunks (prevents splitting mid-sentence)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start += chunk_size - overlap  # Move forward, keep overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process all PDFs\n",
    "print(\"üìñ Reading PDF files...\\n\")\n",
    "all_chunks = []\n",
    "metadata = []\n",
    "\n",
    "for idx, pdf_path in enumerate(PDF_PATHS):\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"‚ö†Ô∏è  File not found - {pdf_path}\")\n",
    "        continue\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    if text:\n",
    "        chunks = chunk_text(text)  # Split into small pieces\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        # Save info about where each chunk came from\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            metadata.append({\n",
    "                \"source\": os.path.basename(pdf_path),\n",
    "                \"chunk_id\": chunk_idx,\n",
    "                \"total_chunks\": len(chunks)\n",
    "            })\n",
    "        \n",
    "        print(f\"  ‚úÖ Created {len(chunks)} pieces\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  No text found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Done!\")\n",
    "print(f\"üìä Total pieces: {len(all_chunks)}\")\n",
    "print(f\"üìè Using chunk size: {CHUNK_SIZE} chars with {OVERLAP} char overlap\")\n",
    "\n",
    "if len(all_chunks) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No text found in PDFs!\")\n",
    "    print(\"Check: File paths correct? PDFs not password-protected?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Create Search Database\n",
    "\n",
    "This creates a searchable database from your PDFs.\n",
    "\n",
    "**Time:** 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Store which PDFs were used for the last answer\nlast_sources_used = []\n\ndef retrieve_relevant_context(query, n_results=NUM_RETRIEVED_DOCS):\n    \"\"\"Find relevant pieces from your PDFs based on the question.\"\"\"\n    global last_sources_used\n    try:\n        # Convert question to searchable numbers\n        query_embedding = embedding_model.encode([query])\n        \n        # Search database for matching pieces\n        results = collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=min(n_results, collection.count())\n        )\n        \n        documents = results['documents'][0] if results['documents'] else []\n        metadatas = results['metadatas'][0] if results['metadatas'] else []\n        \n        # Track which PDFs were used\n        last_sources_used = []\n        if metadatas:\n            seen_sources = set()\n            for meta in metadatas[:3]:  # Only track the 3 pieces we actually use\n                source_name = meta.get('source', 'Unknown')\n                if source_name not in seen_sources:\n                    last_sources_used.append(source_name)\n                    seen_sources.add(source_name)\n        \n        return documents\n    except Exception as e:\n        print(f\"Error searching: {str(e)}\")\n        last_sources_used = []\n        return []\n\ndef generate_response_sync(question, chat_history=None):\n    \"\"\"Get answer from AI using relevant PDF pieces + conversation memory.\"\"\"\n    # Find relevant pieces from PDFs\n    context_docs = retrieve_relevant_context(question)\n    \n    # üéØ TEACHING NOTE: Context Size Settings\n    # We use TOP 3 CHUNKS (out of 7 searched) for better fact-grounding\n    # More chunks = more facts, but can confuse the model if too many\n    # 2000 chars ‚âà 500 tokens ‚âà 1-2 paragraphs of context\n    if context_docs:\n        context_docs = context_docs[:3]  # üìù Use top 3 most relevant chunks\n        context = \"\\n\\n\".join(context_docs)\n        context = context[:2000]  # üìù Limit to 2000 characters maximum\n    else:\n        context = \"No relevant documents found.\"\n    \n    # üéØ TEACHING NOTE: Conversation Memory Implementation\n    # We build a message history to give the AI context of previous exchanges\n    # This allows natural follow-up questions like \"tell me more\" or \"what about X?\"\n    # Memory is limited by CONVERSATION_MEMORY setting to balance context vs speed\n    \n    # Start with system message (persona + grounding instructions + context)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"{PERSONA_DESCRIPTION}\n\nIMPORTANT: Answer using ONLY the specific facts and information from the context below.\nIf the context doesn't contain the answer, say \"I don't have that information in my documents.\"\n\nContext from documents:\n{context}\n\nInstructions:\n- Use actual facts, quotes, and details from the context\n- Cite specific information mentioned in the documents\n- Keep your answer brief and focused (around 50 words)\n- Don't add information not in the context\"\"\"\n        }\n    ]\n    \n    # üéØ TEACHING NOTE: Adding Conversation History\n    # Gradio's chat_history format: [[user_msg1, bot_msg1], [user_msg2, bot_msg2], ...]\n    # We convert this to the API's message format: [{role: user, content: ...}, {role: assistant, content: ...}]\n    # Only include the last N exchanges (controlled by CONVERSATION_MEMORY)\n    \n    if chat_history and CONVERSATION_MEMORY > 0:\n        # Get only the most recent exchanges\n        recent_history = chat_history[-(CONVERSATION_MEMORY):]\n        \n        # Add each exchange to messages\n        for user_msg, bot_msg in recent_history:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n    \n    # Add current question\n    messages.append({\"role\": \"user\", \"content\": question})\n    \n    # üéØ DEBUG OUTPUT - Show conversation memory structure\n    if DEBUG_MEMORY:\n        print(\"\\n\" + \"=\"*80)\n        print(\"üß† DEBUG: CONVERSATION MEMORY\")\n        print(\"=\"*80)\n        print(f\"üìä Sending {len(messages)} messages to HuggingFace API:\")\n        print(f\"   ‚Ä¢ System message: 1\")\n        history_count = (len(messages) - 2) // 2 if len(messages) > 2 else 0\n        print(f\"   ‚Ä¢ History pairs: {history_count} (from last {CONVERSATION_MEMORY} exchanges)\")\n        print(f\"   ‚Ä¢ Current question: 1\")\n        print(f\"\\nüìã Message structure:\")\n        for i, msg in enumerate(messages):\n            role_emoji = \"üñ•Ô∏è\" if msg['role'] == \"system\" else (\"üë§\" if msg['role'] == \"user\" else \"ü§ñ\")\n            content_preview = msg['content'][:60].replace('\\n', ' ').strip()\n            print(f\"   [{i}] {role_emoji} {msg['role']:10} | {content_preview}...\")\n        print(\"=\"*80 + \"\\n\")\n    \n    try:\n        # üéØ TEACHING NOTE: API Call Parameters\n        # - max_tokens: Controls output length (~75 tokens ‚âà 50 words)\n        # - temperature: Controls creativity (0.0=robotic, 1.0=creative)\n        #   Lower temperature = more factual, sticks to documents\n        # - model: Which AI model to use (Meta-Llama-3.1-8B-Instruct)\n        # - messages: Full conversation history (system + previous + current)\n        \n        # Call HuggingFace chat_completion API\n        response = client.chat_completion(\n            messages=messages,\n            model=MODEL_NAME,\n            max_tokens=75,  # üìù ~50 words output (1 token ‚âà 0.75 words)\n            temperature=TEMPERATURE  # üìù From Cell 8, controls creativity\n        )\n        \n        # Extract text from response\n        return response.choices[0].message.content\n        \n    except Exception as e:\n        # Better error handling for HF-specific issues\n        error_str = str(e).lower()\n        \n        if \"503\" in str(e) or \"loading\" in error_str:\n            return \"‚è≥ Model is loading... Please wait 20-30 seconds and try again.\"\n        elif \"model\" in error_str and \"not found\" in error_str:\n            return \"‚ùå Model not available. Try using 'microsoft/Phi-3.5-mini-instruct' in Cell 8\"\n        elif \"429\" in str(e) or \"rate limit\" in error_str:\n            return \"‚ö†Ô∏è Rate limit reached. Wait 10-15 minutes (free tier: ~300 requests/hour)\"\n        else:\n            # Re-raise for async wrapper to handle\n            raise e\n\nasync def generate_response_async(question, chat_history=None, timeout_seconds=30):\n    \"\"\"Wrapper with 30-second timeout to prevent hanging.\"\"\"\n    try:\n        # Run AI call with timeout\n        response_text = await asyncio.wait_for(\n            asyncio.to_thread(generate_response_sync, question, chat_history),\n            timeout=timeout_seconds\n        )\n        return response_text\n    \n    except asyncio.TimeoutError:\n        return \"‚è±Ô∏è **Timeout** - Took too long (>30 seconds). Try a simpler question.\"\n    \n    except Exception as e:\n        error_str = str(e).lower()\n        \n        if \"429\" in str(e) or \"quota\" in error_str or \"rate limit\" in error_str:\n            return \"‚ö†Ô∏è **Rate Limit** - Wait 10-15 minutes and try again.\"\n        elif \"timeout\" in error_str or \"connection\" in error_str:\n            return \"‚ö†Ô∏è **Connection Error** - Check your internet.\"\n        elif \"invalid\" in error_str or \"token\" in error_str or \"authentication\" in error_str or \"401\" in str(e):\n            return \"‚ö†Ô∏è **API Error** - Check your HuggingFace token permissions.\"\n        elif \"503\" in str(e) or \"loading\" in error_str:\n            return \"‚ö†Ô∏è **Model Loading** - The model is waking up. Wait 20-30 seconds and try again.\"\n        else:\n            return f\"‚ùå **Error** - {str(e)[:100]}\"\n\nprint(\"‚úÖ Answer system ready!\")\nprint(\"ü§ñ Using chat_completion API (optimized for conversation)\")\nprint(f\"üß† Conversation memory: {'ENABLED (' + str(CONVERSATION_MEMORY) + ' exchanges)' if CONVERSATION_MEMORY > 0 else 'DISABLED'}\")\nprint(f\"üß™ Debug mode: {'ENABLED üîç (will show message structure in console)' if DEBUG_MEMORY else 'DISABLED (set DEBUG_MEMORY = True in Cell 8 to enable)'}\")\nprint(\"üìè Context: 3 chunks, 2000 chars max\")\nprint(\"üìù Output: ~50 words per response\")\nprint(\"‚è±Ô∏è  Response time: 5-15 seconds\")\nif SHOW_SOURCES:\n    print(\"üìö Source citations enabled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Setup Question Answering\n",
    "\n",
    "This prepares the chatbot to answer your questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Store which PDFs were used for the last answer\nlast_sources_used = []\n\ndef retrieve_relevant_context(query, n_results=NUM_RETRIEVED_DOCS):\n    \"\"\"Find relevant pieces from your PDFs based on the question.\"\"\"\n    global last_sources_used\n    try:\n        # Convert question to searchable numbers\n        query_embedding = embedding_model.encode([query])\n        \n        # Search database for matching pieces\n        results = collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=min(n_results, collection.count())\n        )\n        \n        documents = results['documents'][0] if results['documents'] else []\n        metadatas = results['metadatas'][0] if results['metadatas'] else []\n        \n        # Track which PDFs were used\n        last_sources_used = []\n        if metadatas:\n            seen_sources = set()\n            for meta in metadatas[:3]:  # Only track the 3 pieces we actually use\n                source_name = meta.get('source', 'Unknown')\n                if source_name not in seen_sources:\n                    last_sources_used.append(source_name)\n                    seen_sources.add(source_name)\n        \n        return documents\n    except Exception as e:\n        print(f\"Error searching: {str(e)}\")\n        last_sources_used = []\n        return []\n\ndef generate_response_sync(question, chat_history=None):\n    \"\"\"Get answer from AI using relevant PDF pieces + conversation memory.\"\"\"\n    # Find relevant pieces from PDFs\n    context_docs = retrieve_relevant_context(question)\n    \n    # üéØ TEACHING NOTE: Context Size Settings\n    # We use TOP 3 CHUNKS (out of 7 searched) for better fact-grounding\n    # More chunks = more facts, but can confuse the model if too many\n    # 2000 chars ‚âà 500 tokens ‚âà 1-2 paragraphs of context\n    if context_docs:\n        context_docs = context_docs[:3]  # üìù Use top 3 most relevant chunks\n        context = \"\\n\\n\".join(context_docs)\n        context = context[:2000]  # üìù Limit to 2000 characters maximum\n    else:\n        context = \"No relevant documents found.\"\n    \n    # üéØ TEACHING NOTE: Conversation Memory Implementation\n    # We build a message history to give the AI context of previous exchanges\n    # This allows natural follow-up questions like \"tell me more\" or \"what about X?\"\n    # Memory is limited by CONVERSATION_MEMORY setting to balance context vs speed\n    \n    # Start with system message (persona + grounding instructions + context)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"{PERSONA_DESCRIPTION}\n\nIMPORTANT: Answer using ONLY the specific facts and information from the context below.\nIf the context doesn't contain the answer, say \"I don't have that information in my documents.\"\n\nContext from documents:\n{context}\n\nInstructions:\n- Use actual facts, quotes, and details from the context\n- Cite specific information mentioned in the documents\n- Keep your answer brief and focused (around 50 words)\n- Don't add information not in the context\"\"\"\n        }\n    ]\n    \n    # üéØ TEACHING NOTE: Adding Conversation History\n    # Gradio's chat_history format: [[user_msg1, bot_msg1], [user_msg2, bot_msg2], ...]\n    # We convert this to the API's message format: [{role: user, content: ...}, {role: assistant, content: ...}]\n    # Only include the last N exchanges (controlled by CONVERSATION_MEMORY)\n    \n    if chat_history and CONVERSATION_MEMORY > 0:\n        # Get only the most recent exchanges\n        recent_history = chat_history[-(CONVERSATION_MEMORY):]\n        \n        # Add each exchange to messages\n        for user_msg, bot_msg in recent_history:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n    \n    # Add current question\n    messages.append({\"role\": \"user\", \"content\": question})\n    \n    try:\n        # üéØ TEACHING NOTE: API Call Parameters\n        # - max_tokens: Controls output length (~75 tokens ‚âà 50 words)\n        # - temperature: Controls creativity (0.0=robotic, 1.0=creative)\n        #   Lower temperature = more factual, sticks to documents\n        # - model: Which AI model to use (Meta-Llama-3.1-8B-Instruct)\n        # - messages: Full conversation history (system + previous + current)\n        \n        # Call HuggingFace chat_completion API\n        response = client.chat_completion(\n            messages=messages,\n            model=MODEL_NAME,\n            max_tokens=75,  # üìù ~50 words output (1 token ‚âà 0.75 words)\n            temperature=TEMPERATURE  # üìù From Cell 8, controls creativity\n        )\n        \n        # Extract text from response\n        return response.choices[0].message.content\n        \n    except Exception as e:\n        # Better error handling for HF-specific issues\n        error_str = str(e).lower()\n        \n        if \"503\" in str(e) or \"loading\" in error_str:\n            return \"‚è≥ Model is loading... Please wait 20-30 seconds and try again.\"\n        elif \"model\" in error_str and \"not found\" in error_str:\n            return \"‚ùå Model not available. Try using 'microsoft/Phi-3.5-mini-instruct' in Cell 8\"\n        elif \"429\" in str(e) or \"rate limit\" in error_str:\n            return \"‚ö†Ô∏è Rate limit reached. Wait 10-15 minutes (free tier: ~300 requests/hour)\"\n        else:\n            # Re-raise for async wrapper to handle\n            raise e\n\nasync def generate_response_async(question, chat_history=None, timeout_seconds=30):\n    \"\"\"Wrapper with 30-second timeout to prevent hanging.\"\"\"\n    try:\n        # Run AI call with timeout\n        response_text = await asyncio.wait_for(\n            asyncio.to_thread(generate_response_sync, question, chat_history),\n            timeout=timeout_seconds\n        )\n        return response_text\n    \n    except asyncio.TimeoutError:\n        return \"‚è±Ô∏è **Timeout** - Took too long (>30 seconds). Try a simpler question.\"\n    \n    except Exception as e:\n        error_str = str(e).lower()\n        \n        if \"429\" in str(e) or \"quota\" in error_str or \"rate limit\" in error_str:\n            return \"‚ö†Ô∏è **Rate Limit** - Wait 10-15 minutes and try again.\"\n        elif \"timeout\" in error_str or \"connection\" in error_str:\n            return \"‚ö†Ô∏è **Connection Error** - Check your internet.\"\n        elif \"invalid\" in error_str or \"token\" in error_str or \"authentication\" in error_str or \"401\" in str(e):\n            return \"‚ö†Ô∏è **API Error** - Check your HuggingFace token permissions.\"\n        elif \"503\" in str(e) or \"loading\" in error_str:\n            return \"‚ö†Ô∏è **Model Loading** - The model is waking up. Wait 20-30 seconds and try again.\"\n        else:\n            return f\"‚ùå **Error** - {str(e)[:100]}\"\n\nprint(\"‚úÖ Answer system ready!\")\nprint(\"ü§ñ Using chat_completion API (optimized for conversation)\")\nprint(f\"üß† Conversation memory: {'ENABLED (' + str(CONVERSATION_MEMORY) + ' exchanges)' if CONVERSATION_MEMORY > 0 else 'DISABLED'}\")\nprint(\"üìè Context: 3 chunks, 2000 chars max\")\nprint(\"üìù Output: ~50 words per response\")\nprint(\"‚è±Ô∏è  Response time: 5-15 seconds\")\nif SHOW_SOURCES:\n    print(\"üìö Source citations enabled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8B: Initialize Empathy Analyzer\n",
    "\n",
    "This sets up the empathy tracking system that will analyze your messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMPATHY ANALYZER - Tracks 5 dimensions of empathic communication\n",
    "# ============================================================================\n",
    "\n",
    "class EmpathyAnalyzer:\n",
    "    \"\"\"Analyzes user messages for empathy across 5 dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        self.user_messages = []\n",
    "        self.empathy_scores = []\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Empathy linguistic markers\n",
    "        self.open_question_words = ['how', 'what', 'why', 'tell', 'describe', 'explain']\n",
    "        self.emotion_words = [\n",
    "            'feel', 'feeling', 'felt', 'emotion', 'happy', 'sad', 'angry', \n",
    "            'frustrated', 'worried', 'anxious', 'excited', 'disappointed',\n",
    "            'upset', 'hurt', 'joy', 'fear', 'surprise', 'disgust', 'content',\n",
    "            'grateful', 'proud', 'ashamed', 'guilty', 'nervous', 'scared'\n",
    "        ]\n",
    "        self.perspective_phrases = [\n",
    "            'you feel', 'you might', 'from your', 'in your', 'your perspective',\n",
    "            'you seem', 'you appear', 'you sound', 'for you', 'to you',\n",
    "            'you think', 'you believe', 'you experience', 'your view'\n",
    "        ]\n",
    "        self.active_listening_phrases = [\n",
    "            'tell me more', 'i understand', 'i hear', 'i see', 'help me understand',\n",
    "            'that makes sense', 'i appreciate', 'thank you for sharing',\n",
    "            'go on', 'continue', 'interesting', 'i get it', 'i follow'\n",
    "        ]\n",
    "    \n",
    "    def analyze_message(self, message):\n",
    "        \"\"\"Analyze a single message for empathy markers.\"\"\"\n",
    "        message_lower = message.lower()\n",
    "        \n",
    "        # 1. Sentiment/Warmth (0-20 points)\n",
    "        sentiment = self.vader.polarity_scores(message)\n",
    "        warmth_score = max(0, min(20, (sentiment['compound'] + 1) * 10))\n",
    "        \n",
    "        # 2. Open Questions (0-20 points)\n",
    "        open_question_count = sum(1 for word in self.open_question_words if word in message_lower)\n",
    "        has_question = '?' in message\n",
    "        open_score = min(20, open_question_count * 10) if has_question else 0\n",
    "        \n",
    "        # 3. Emotion Words (0-20 points)\n",
    "        emotion_count = sum(1 for word in self.emotion_words if word in message_lower)\n",
    "        emotion_score = min(20, emotion_count * 7)\n",
    "        \n",
    "        # 4. Perspective-Taking (0-20 points)\n",
    "        perspective_count = sum(1 for phrase in self.perspective_phrases if phrase in message_lower)\n",
    "        perspective_score = min(20, perspective_count * 10)\n",
    "        \n",
    "        # 5. Active Listening (0-20 points)\n",
    "        listening_count = sum(1 for phrase in self.active_listening_phrases if phrase in message_lower)\n",
    "        listening_score = min(20, listening_count * 7)\n",
    "        \n",
    "        # Total score\n",
    "        total_score = warmth_score + open_score + emotion_score + perspective_score + listening_score\n",
    "        \n",
    "        return {\n",
    "            'message': message,\n",
    "            'warmth': warmth_score,\n",
    "            'open_questions': open_score,\n",
    "            'emotion_words': emotion_score,\n",
    "            'perspective_taking': perspective_score,\n",
    "            'active_listening': listening_score,\n",
    "            'total': total_score,\n",
    "            'sentiment_raw': sentiment['compound']\n",
    "        }\n",
    "    \n",
    "    def add_user_message(self, message, bot_response):\n",
    "        \"\"\"Track a user message and bot response.\"\"\"\n",
    "        analysis = self.analyze_message(message)\n",
    "        self.user_messages.append(message)\n",
    "        self.empathy_scores.append(analysis)\n",
    "        self.conversation_history.append({\n",
    "            'user': message,\n",
    "            'bot': bot_response,\n",
    "            'empathy': analysis\n",
    "        })\n",
    "    \n",
    "    def get_average_scores(self):\n",
    "        \"\"\"Calculate average scores across all messages.\"\"\"\n",
    "        if not self.empathy_scores:\n",
    "            return None\n",
    "        \n",
    "        n = len(self.empathy_scores)\n",
    "        return {\n",
    "            'warmth': sum(s['warmth'] for s in self.empathy_scores) / n,\n",
    "            'open_questions': sum(s['open_questions'] for s in self.empathy_scores) / n,\n",
    "            'emotion_words': sum(s['emotion_words'] for s in self.empathy_scores) / n,\n",
    "            'perspective_taking': sum(s['perspective_taking'] for s in self.empathy_scores) / n,\n",
    "            'active_listening': sum(s['active_listening'] for s in self.empathy_scores) / n,\n",
    "            'total': sum(s['total'] for s in self.empathy_scores) / n,\n",
    "            'message_count': n\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive empathy report.\"\"\"\n",
    "        if len(self.empathy_scores) < 10:\n",
    "            return None\n",
    "        \n",
    "        avg = self.get_average_scores()\n",
    "        total_score = avg['total']\n",
    "        \n",
    "        # Interpretation\n",
    "        if total_score >= 80:\n",
    "            interpretation = \"Excellent - Consistently demonstrates empathic responses\"\n",
    "        elif total_score >= 60:\n",
    "            interpretation = \"Good - Regular empathic responses with room to grow\"\n",
    "        elif total_score >= 40:\n",
    "            interpretation = \"Moderate - Awareness of emotions but inconsistent\"\n",
    "        elif total_score >= 20:\n",
    "            interpretation = \"Developing - Beginning to recognize emotions\"\n",
    "        else:\n",
    "            interpretation = \"Needs Practice - Focus on foundational skills\"\n",
    "        \n",
    "        # Recommendations\n",
    "        recommendations = []\n",
    "        if avg['warmth'] < 15:\n",
    "            recommendations.append(\"Use warmer, more supportive language\")\n",
    "        if avg['open_questions'] < 15:\n",
    "            recommendations.append(\"Ask more open-ended questions (what/how/why)\")\n",
    "        if avg['emotion_words'] < 15:\n",
    "            recommendations.append(\"Acknowledge emotions more explicitly\")\n",
    "        if avg['perspective_taking'] < 15:\n",
    "            recommendations.append(\"Practice perspective-taking phrases\")\n",
    "        if avg['active_listening'] < 15:\n",
    "            recommendations.append(\"Show more active listening markers\")\n",
    "        \n",
    "        # Format report\n",
    "        report = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë           EMPATHY TRAINING ANALYSIS REPORT                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üìä OVERALL EMPATHY SCORE: {total_score:.1f}/100\n",
    "   {interpretation}\n",
    "\n",
    "üìà DIMENSION BREAKDOWN:\n",
    "   ‚Ä¢ Sentiment/Warmth:      {avg['warmth']:.1f}/20 {'‚úÖ' if avg['warmth'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Open Questions:        {avg['open_questions']:.1f}/20 {'‚úÖ' if avg['open_questions'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Emotion Recognition:   {avg['emotion_words']:.1f}/20 {'‚úÖ' if avg['emotion_words'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Perspective-Taking:    {avg['perspective_taking']:.1f}/20 {'‚úÖ' if avg['perspective_taking'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Active Listening:      {avg['active_listening']:.1f}/20 {'‚úÖ' if avg['active_listening'] >= 15 else '‚ö†Ô∏è'}\n",
    "\n",
    "üìâ CONVERSATION METRICS:\n",
    "   ‚Ä¢ Total Messages Analyzed: {avg['message_count']}\n",
    "   ‚Ä¢ Average Sentiment: {sum(s['sentiment_raw'] for s in self.empathy_scores) / len(self.empathy_scores):.2f} (-1 to +1)\n",
    "   ‚Ä¢ Questions Asked: {sum(1 for s in self.empathy_scores if s['open_questions'] > 0)}\n",
    "   ‚Ä¢ Emotion Words Used: {sum(1 for s in self.empathy_scores if s['emotion_words'] > 0)} messages\n",
    "\n",
    "üí° RECOMMENDATIONS FOR IMPROVEMENT:\n",
    "\"\"\"\n",
    "        if recommendations:\n",
    "            for rec in recommendations:\n",
    "                report += f\"   ‚Ä¢ {rec}\\n\"\n",
    "        else:\n",
    "            report += \"   ‚Ä¢ Great work! Keep practicing to maintain your skills\\n\"\n",
    "        \n",
    "        report += \"\\n‚úÖ Report complete - Keep practicing empathic communication!\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def export_to_csv(self):\n",
    "        \"\"\"Export conversation data to CSV format.\"\"\"\n",
    "        import csv\n",
    "        from io import StringIO\n",
    "        \n",
    "        output = StringIO()\n",
    "        writer = csv.writer(output)\n",
    "        \n",
    "        # Header\n",
    "        writer.writerow([\n",
    "            'Message #', 'User Message', 'Bot Response', \n",
    "            'Warmth', 'Open Questions', 'Emotion Words', \n",
    "            'Perspective-Taking', 'Active Listening', 'Total Score'\n",
    "        ])\n",
    "        \n",
    "        # Data\n",
    "        for i, conv in enumerate(self.conversation_history, 1):\n",
    "            emp = conv['empathy']\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                conv['user'],\n",
    "                conv['bot'],\n",
    "                f\"{emp['warmth']:.1f}\",\n",
    "                f\"{emp['open_questions']:.1f}\",\n",
    "                f\"{emp['emotion_words']:.1f}\",\n",
    "                f\"{emp['perspective_taking']:.1f}\",\n",
    "                f\"{emp['active_listening']:.1f}\",\n",
    "                f\"{emp['total']:.1f}\"\n",
    "            ])\n",
    "        \n",
    "        return output.getvalue()\n",
    "\n",
    "# Initialize global empathy analyzer\n",
    "empathy_analyzer = EmpathyAnalyzer()\n",
    "\n",
    "print(\"‚úÖ Empathy Analyzer ready!\")\n",
    "print(\"üìä Tracking 5 dimensions:\")\n",
    "print(\"   1. Sentiment/Warmth (positive emotional tone)\")\n",
    "print(\"   2. Open Questions (exploration)\")\n",
    "print(\"   3. Emotion Recognition (naming feelings)\")\n",
    "print(\"   4. Perspective-Taking (seeing their view)\")\n",
    "print(\"   5. Active Listening (engagement)\")\n",
    "print(\"\\nüìù Report will generate after 10 messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def chat_interface(message, history):\n    \"\"\"Handle chat messages with empathy tracking, conversation memory, and source citations.\"\"\"\n    # Get answer from AI (with conversation memory if enabled)\n    response = await generate_response_async(message, history)\n    \n    # Add source citations if enabled\n    if SHOW_SOURCES and last_sources_used:\n        response += \"\\n\\n---\\n**üìö Sources:**\\n\"\n        for i, source in enumerate(last_sources_used, 1):\n            response += f\"{i}. {source}\\n\"\n    \n    # Track empathy (user message + bot response)\n    empathy_analyzer.add_user_message(message, response)\n    \n    # Check if we've reached 10 messages - generate report\n    message_count = len(empathy_analyzer.user_messages)\n    if message_count == 10:\n        report = empathy_analyzer.generate_report()\n        if report:\n            response += \"\\n\\n\" + \"=\"*60 + \"\\n\"\n            response += report\n            response += \"\\n\" + \"=\"*60\n            response += \"\\n\\nüíæ **Want to save your data?** Run the export cell below to download as CSV.\"\n    elif message_count < 10:\n        # Show progress\n        remaining = 10 - message_count\n        response += f\"\\n\\n_üìä Empathy tracking: {message_count}/10 messages ({remaining} more for report)_\"\n    \n    return response\n\n# ============================================================================\n# STARTER QUESTIONS - OPTIONAL ‚úèÔ∏è\n# ============================================================================\n# These appear as clickable examples when chat starts\n# Change these to match your PDFs and persona\n\nSTARTER_QUESTIONS = [\n    \"What are your main beliefs or values?\",\n    \"How did that experience make you feel?\",\n    \"Tell me more about your perspective on this topic.\",\n    \"You seem passionate about this - what drives that feeling?\",\n    \"From your point of view, what are your greatest achievements?\",\n]\n\n# Create chat interface\nmemory_status = f\"üß† Conversation memory: {'ENABLED (' + str(CONVERSATION_MEMORY) + ' message pairs)' if CONVERSATION_MEMORY > 0 else 'DISABLED (each question treated independently)'}\"\n\ndemo = gr.ChatInterface(\n    fn=chat_interface,\n    title=f\"ü§ñ Chat with {PERSONA_NAME} - Empathy Training\",\n    description=f\"\"\"Practice empathic conversation with {PERSONA_NAME}.\n    \n    üìä **Empathy Assessment Enabled**\n    - Your messages are analyzed for empathy markers\n    - Report generated after 10 messages\n    - Track: warmth, questions, emotions, perspective, listening\n    \n    {memory_status}\n    {'üìñ Source citations enabled - see which PDFs were used' if SHOW_SOURCES else ''}\n    \n    ‚è±Ô∏è Response time: 5-15 seconds\n    \"\"\",\n    examples=STARTER_QUESTIONS,\n)\n\n# Launch chat\nprint(\"=\" * 80)\nprint(\"üöÄ LAUNCHING EMPATHY TRAINING CHAT\")\nprint(\"=\" * 80)\nprint(\"\\nüìä EMPATHY ASSESSMENT ACTIVE\")\nprint(\"   ‚Ä¢ Tracking 5 empathy dimensions\")\nprint(\"   ‚Ä¢ Report after 10 messages\")\nprint(\"   ‚Ä¢ CSV export available\\n\")\nprint(f\"\\nüß† CONVERSATION MEMORY: {'ENABLED (' + str(CONVERSATION_MEMORY) + ' exchanges)' if CONVERSATION_MEMORY > 0 else 'DISABLED'}\")\nif CONVERSATION_MEMORY > 0:\n    print(f\"   ‚Ä¢ Bot remembers last {CONVERSATION_MEMORY} message pairs\")\n    print(\"   ‚Ä¢ Follow-up questions work naturally\")\n    print(\"   ‚Ä¢ Change CONVERSATION_MEMORY in Cell 8 to adjust\\n\")\nelse:\n    print(\"   ‚Ä¢ Each question treated independently\")\n    print(\"   ‚Ä¢ Set CONVERSATION_MEMORY > 0 in Cell 8 to enable\\n\")\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Use the PUBLIC LINK below (not Colab interface)\\n\")\nif SHOW_SOURCES:\n    print(\"üìö Sources ON - answers show which PDFs were used\\n\")\nprint(\"üëá COPY THIS LINK AND OPEN IN NEW TAB:\\n\")\n\ndemo.launch(\n    share=True,      # Create public link\n    inline=False,    # Don't show in Colab (unstable)\n    debug=True       # Show errors\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Chat is live with empathy tracking + conversation memory!\")\nprint(\"=\" * 80)\nprint(\"\\nüìå STEPS:\")\nprint(\"   1. Find 'Running on public URL:' above\")\nprint(\"   2. Copy the https://xxxxx.gradio.live link\")\nprint(\"   3. Open in new browser tab\")\nprint(\"   4. Start chatting empathically!\")\nif CONVERSATION_MEMORY > 0:\n    print(f\"   5. Try follow-up questions (bot remembers last {CONVERSATION_MEMORY} exchanges)\")\n    print(\"   6. After 10 messages, view your empathy report\")\nelse:\n    print(\"   5. After 10 messages, view your empathy report\")\nif SHOW_SOURCES:\n    print(f\"   {'7' if CONVERSATION_MEMORY > 0 else '6'}. Check bottom of answers for sources\")\nprint(\"\\nüí° Link expires after 72 hours of no use\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Step 9: Export Conversation Data (Optional)\n",
    "\n",
    "**Run this after completing your conversation** to download your empathy data as CSV.\n",
    "\n",
    "This will create a file with:\n",
    "- All your messages and bot responses\n",
    "- Empathy scores for each dimension\n",
    "- Total empathy score per message\n",
    "\n",
    "You can open this in Excel or Google Sheets for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export conversation data to CSV\n",
    "if len(empathy_analyzer.conversation_history) > 0:\n",
    "    print(\"üì• Exporting conversation data...\\n\")\n",
    "    \n",
    "    csv_data = empathy_analyzer.export_to_csv()\n",
    "    \n",
    "    # Save to file\n",
    "    from google.colab import files\n",
    "    import datetime\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"empathy_conversation_{timestamp}.csv\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(csv_data)\n",
    "    \n",
    "    print(f\"‚úÖ Data exported to: {filename}\")\n",
    "    print(f\"üìä Total messages: {len(empathy_analyzer.user_messages)}\")\n",
    "    \n",
    "    avg_scores = empathy_analyzer.get_average_scores()\n",
    "    if avg_scores:\n",
    "        print(f\"üìà Average empathy score: {avg_scores['total']:.1f}/100\")\n",
    "    \n",
    "    print(\"\\nüì• Downloading file...\")\n",
    "    files.download(filename)\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "    print(\"\\nüí° You can now open this CSV file in Excel or Google Sheets\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No conversation data to export yet!\")\n",
    "    print(\"üí¨ Have a conversation first, then run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Step 10: Start New Conversation (Optional)\n",
    "\n",
    "**Run this to practice empathy again** with a fresh conversation.\n",
    "\n",
    "This will:\n",
    "- Reset the empathy tracker (0/10 messages)\n",
    "- Clear previous conversation history\n",
    "- Launch a new chat interface\n",
    "\n",
    "**üí° Tip:** Export your current conversation (Step 9) BEFORE running this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# RESET & START NEW CONVERSATION\n# ============================================================================\n\nprint(\"üîÑ Resetting empathy tracker...\\n\")\n\n# Reinitialize empathy analyzer (clears all previous data)\nempathy_analyzer = EmpathyAnalyzer()\n\nprint(\"‚úÖ Empathy tracker reset!\")\nprint(\"   ‚Ä¢ Message counter: 0/10\")\nprint(\"   ‚Ä¢ Previous conversation cleared\")\nprint(f\"   ‚Ä¢ Memory setting: {CONVERSATION_MEMORY} exchanges\")\nprint(\"   ‚Ä¢ Ready for fresh practice\\n\")\n\n# Relaunch chat interface\nprint(\"=\" * 80)\nprint(\"üöÄ LAUNCHING NEW EMPATHY TRAINING CHAT\")\nprint(\"=\" * 80)\nprint(\"\\nüìä EMPATHY ASSESSMENT ACTIVE\")\nprint(\"   ‚Ä¢ Tracking 5 empathy dimensions\")\nprint(\"   ‚Ä¢ Report after 10 messages\")\nprint(\"   ‚Ä¢ CSV export available\\n\")\nprint(f\"\\nüß† CONVERSATION MEMORY: {'ENABLED (' + str(CONVERSATION_MEMORY) + ' exchanges)' if CONVERSATION_MEMORY > 0 else 'DISABLED'}\")\nif CONVERSATION_MEMORY > 0:\n    print(f\"   ‚Ä¢ Bot remembers last {CONVERSATION_MEMORY} message pairs\")\n    print(\"   ‚Ä¢ Follow-up questions work naturally\")\n    print(\"   ‚Ä¢ Change CONVERSATION_MEMORY in Cell 8 to adjust\\n\")\nelse:\n    print(\"   ‚Ä¢ Each question treated independently\")\n    print(\"   ‚Ä¢ Set CONVERSATION_MEMORY > 0 in Cell 8 to enable\\n\")\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Use the PUBLIC LINK below (not Colab interface)\\n\")\nif SHOW_SOURCES:\n    print(\"üìö Sources ON - answers show which PDFs were used\\n\")\nprint(\"üëá COPY THIS LINK AND OPEN IN NEW TAB:\\n\")\n\ndemo.launch(\n    share=True,      # Create public link\n    inline=False,    # Don't show in Colab (unstable)\n    debug=True       # Show errors\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ New conversation started!\")\nprint(\"=\" * 80)\nprint(\"\\nüìå STEPS:\")\nprint(\"   1. Find 'Running on public URL:' above\")\nprint(\"   2. Copy the https://xxxxx.gradio.live link\")\nprint(\"   3. Open in new browser tab\")\nprint(\"   4. Start your new empathy practice!\")\nif CONVERSATION_MEMORY > 0:\n    print(f\"   5. Try follow-up questions (bot remembers last {CONVERSATION_MEMORY} exchanges)\")\nprint(\"\\nüí° Remember: Export your previous conversation first if you haven't already\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### API Token Issues:\n",
    "- **Error: \"Invalid token\"**\n",
    "  - Get a new token from: https://huggingface.co/settings/tokens\n",
    "  - Make sure you copied the entire token (starts with `hf_`)\n",
    "  - Replace `YOUR_API_TOKEN_HERE` in Step 4\n",
    "\n",
    "### Rate Limit Issues:\n",
    "- **Error: \"Rate limit reached\"**\n",
    "  - Free tier: ~300 requests/hour\n",
    "  - Wait 10-15 minutes before trying again\n",
    "  - Consider upgrading to PRO ($9/month) for 20x more requests\n",
    "\n",
    "### PDF Issues:\n",
    "- **\"File not found\" errors:**\n",
    "  - Check that Google Drive is mounted (Step 3)\n",
    "  - Verify PDF file paths are correct\n",
    "  - Make sure paths start with `/content/drive/MyDrive/`\n",
    "  \n",
    "- **\"No text extracted\":**\n",
    "  - PDF might be scanned images (not searchable text)\n",
    "  - PDF might be password-protected\n",
    "  - Try opening the PDF to verify it has selectable text\n",
    "\n",
    "### Response Issues:\n",
    "- **Responses don't match persona:**\n",
    "  - Make `PERSONA_DESCRIPTION` more detailed and specific\n",
    "  - Add more example phrases/words they use\n",
    "  \n",
    "- **Responses aren't relevant:**\n",
    "  - Increase `NUM_RETRIEVED_DOCS` (try 5 or 7)\n",
    "  - Make sure PDFs contain information about the topic\n",
    "  - Ask more specific questions\n",
    "\n",
    "### Model Issues:\n",
    "- **Slow responses:**\n",
    "  - Try a smaller model (microsoft/Phi-3-mini-4k-instruct)\n",
    "  - Reduce MAX_OUTPUT_TOKENS\n",
    "  \n",
    "- **Model not available:**\n",
    "  - Check model name at https://huggingface.co/models\n",
    "  - Try alternative models listed in Cell 8\n",
    "\n",
    "### Performance Issues:\n",
    "- **Colab disconnects or times out:**\n",
    "  - This is normal for free Colab after ~12 hours\n",
    "  - Save your work and restart\n",
    "  - Keep the browser tab active\n",
    "\n",
    "### Need More Help?\n",
    "- Check HuggingFace status: https://status.huggingface.co/\n",
    "- HuggingFace documentation: https://huggingface.co/docs/api-inference/\n",
    "- Verify free tier limits at: https://huggingface.co/pricing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}