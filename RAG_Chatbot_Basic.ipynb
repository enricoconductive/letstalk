{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot - Simple Version\n",
    "\n",
    "## What This Does\n",
    "\n",
    "This notebook creates a chatbot that answers questions about your PDF documents.\n",
    "\n",
    "**You Need:**\n",
    "- PDF files in your Google Drive\n",
    "- A free Google Gemini API key\n",
    "- Internet connection\n",
    "\n",
    "**Cost:** 100% FREE\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** See STUDENT_GUIDE.md for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Libraries\n",
    "\n",
    "This installs the required tools. Takes about 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q chromadb gradio pypdf sentence-transformers google-generativeai vaderSentiment\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"\\n‚ÑπÔ∏è  Note: You may see dependency warnings about 'opentelemetry' packages.\")\n",
    "print(\"   These are non-critical and won't affect functionality. You can safely ignore them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load Libraries\n",
    "\n",
    "Load the tools we just installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from google.colab import drive\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Connect Google Drive\n",
    "\n",
    "**Steps:**\n",
    "1. Click the link that appears\n",
    "2. Choose your Google account\n",
    "3. Click \"Allow\"\n",
    "\n",
    "Your files will be at: `/content/drive/MyDrive/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(\"üìÅ Your files are available at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Configuration - CHANGE THESE! ‚úèÔ∏è\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: You must edit the values below**\n",
    "\n",
    "### Get Your FREE API Key:\n",
    "1. Go to: https://aistudio.google.com/app/apikey\n",
    "2. Click \"Create API Key\"\n",
    "3. Copy the key\n",
    "4. Paste it in the next cell where it says `YOUR_API_KEY_HERE`\n",
    "\n",
    "### Add Your PDF Files:\n",
    "- Upload PDFs to your Google Drive\n",
    "- Update the `PDF_PATHS` list with your file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# 1. API KEY - CHANGE THIS! ‚úèÔ∏è\n# ============================================================================\n# Get your key from: https://aistudio.google.com/app/apikey\n\nGEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # ‚Üê PASTE YOUR KEY HERE\n\n# ============================================================================\n# 2. PERSONA - CUSTOMIZE THIS! ‚úèÔ∏è\n# ============================================================================\n\nPERSONA_NAME = \"Your Persona Name\"  # ‚úèÔ∏è CHANGE THIS - e.g., \"Albert Einstein\", \"Oprah Winfrey\"\n\n# ‚úèÔ∏è CUSTOMIZE THIS: Describe your persona's speaking style and personality\nPERSONA_DESCRIPTION = \"\"\"\nReplace this entire section with your persona's description.\n\nTemplate:\nYou are [NAME], [brief description/title/role].\nYou speak in a [adjective] manner, using [characteristic words/phrases].\nYou emphasize [what they care about] and often [communication patterns].\n\nInstructions for customization:\n- Describe HOW they speak (tone, word choice, sentence structure)\n- Include specific phrases or words they commonly use\n- Mention what topics/themes they emphasize\n- Note any unique speaking patterns or habits\n- Keep it focused on communication style, not just biographical facts\n\nExample:\n\"You are Marie Curie, pioneering scientist. You speak precisely and scientifically,\nusing terms like 'research,' 'experiment,' and 'discovery.' You emphasize evidence-based\nreasoning and the importance of persistence in scientific work.\"\n\"\"\"\n\n# Tip: Describe how your person speaks and thinks\n\n# ============================================================================\n# 3. RESPONSE SETTINGS - OPTIONAL\n# ============================================================================\n\nTEMPERATURE = 0.7  # Creativity level (0.0 = focused, 1.0 = creative)\n\nMAX_OUTPUT_TOKENS = 500  # Maximum response length (~375 words)\n\nNUM_RETRIEVED_DOCS = 7  # How many document pieces to search\n\n# ============================================================================\n# 4. SOURCE CITATIONS - OPTIONAL\n# ============================================================================\n\nSHOW_SOURCES = True  # True = show which PDFs were used, False = hide\n\n# ============================================================================\n# 5. CHUNKING SETTINGS - OPTIONAL\n# ============================================================================\n# How to split your PDFs into searchable pieces\n\nCHUNK_SIZE = 1000  # Characters per chunk (500-2000 recommended)\nOVERLAP = 200      # Overlap between chunks (keeps context)\n\n# üí° EXAMPLES - When to adjust:\n#\n# Example 1: SHORT & PRECISE (for quick facts)\n#   CHUNK_SIZE = 500\n#   OVERLAP = 100\n#   ‚úÖ Best for: Short Q&A, specific facts, definitions\n#   ‚úÖ Pros: Fast, precise answers\n#   ‚ùå Cons: May miss broader context\n#\n# Example 2: LONG & CONTEXTUAL (for complex topics)\n#   CHUNK_SIZE = 1500\n#   OVERLAP = 300\n#   ‚úÖ Best for: Detailed explanations, complex reasoning\n#   ‚úÖ Pros: Rich context, complete thoughts\n#   ‚ùå Cons: Slower, may include irrelevant info\n#\n# üéØ CURRENT (BALANCED): 1000 chars, 200 overlap\n#   ‚úÖ Works well for general conversation and empathy training\n\n# ============================================================================\n# 6. PDF FILES - CHANGE THIS! ‚úèÔ∏è\n# ============================================================================\n# Format: \"/content/drive/MyDrive/folder_name/file_name.pdf\"\n\nPDF_PATHS = [\n    \"/content/drive/MyDrive/your_folder/document1.pdf\",  # ‚Üê CHANGE THESE\n    \"/content/drive/MyDrive/your_folder/document2.pdf\",  # ‚Üê TO YOUR PATHS\n    \"/content/drive/MyDrive/your_folder/document3.pdf\",\n    # Add more files as needed\n]\n\n# ============================================================================\n# SETUP (Don't change this part)\n# ============================================================================\ngenai.configure(api_key=GEMINI_API_KEY)\nmodel = genai.GenerativeModel('gemini-2.0-flash')  # Fast & free AI model\n\nprint(\"‚úÖ Configuration complete!\")\nprint(f\"üìã Persona: {PERSONA_NAME}\")\nprint(f\"ü§ñ Model: gemini-2.0-flash\")\nprint(f\"üìÑ PDF files: {len(PDF_PATHS)}\")\nprint(f\"üå°Ô∏è  Creativity: {TEMPERATURE}\")\nprint(f\"üìä Search pieces: {NUM_RETRIEVED_DOCS}\")\nprint(f\"üìö Show sources: {'ON ‚úÖ' if SHOW_SOURCES else 'OFF'}\")\nprint(f\"üìè Chunk size: {CHUNK_SIZE} chars (overlap: {OVERLAP})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 5: Test API Connection\n",
    "\n",
    "**Run this first!** This checks if your API key works.\n",
    "\n",
    "‚úÖ If successful: Continue to next step  \n",
    "‚ùå If failed: Check your API key and try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üß™ Testing API connection...\")\nprint(\"=\" * 60)\n\ntry:\n    # Simple test prompt\n    test_response = model.generate_content(\n        \"Say 'Hello! API is working!' in a friendly, enthusiastic style.\",\n        generation_config=genai.types.GenerationConfig(\n            temperature=0.7,\n            max_output_tokens=50,\n        ),\n    )\n    \n    print(\"‚úÖ SUCCESS! API is working!\")\n    print(f\"\\nTest Response: {test_response.text}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚úÖ You can proceed with the rest of the notebook!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå API TEST FAILED!\")\n    print(f\"Error: {str(e)}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚ö†Ô∏è  STOP! Fix this issue before proceeding:\")\n    print(\"  1. Check your API key is correct\")\n    print(\"  2. Check your internet connection\")\n    print(\"  3. Visit https://aistudio.google.com/app/apikey to verify your key\")\n    print(\"  4. Check API status at https://status.cloud.google.com/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Read PDF Files\n",
    "\n",
    "This reads your PDFs and splits them into small pieces for searching.\n",
    "\n",
    "**Time:** 1-2 minutes depending on file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_text_from_pdf(pdf_path):\n    \"\"\"Get text from a PDF file.\"\"\"\n    try:\n        reader = PdfReader(pdf_path)\n        text = \"\"\n        for page in reader.pages:\n            text += page.extract_text() + \"\\n\"\n        return text\n    except Exception as e:\n        print(f\"‚ùå Error reading {pdf_path}: {str(e)}\")\n        return \"\"\n\ndef chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n    \"\"\"Split text into small pieces (chunks) for better searching.\n    \n    Uses settings from Cell 8:\n    - chunk_size: Characters per chunk\n    - overlap: Characters that overlap between chunks (prevents splitting mid-sentence)\n    \"\"\"\n    chunks = []\n    start = 0\n    \n    while start < len(text):\n        end = start + chunk_size\n        chunk = text[start:end]\n        \n        if chunk.strip():\n            chunks.append(chunk)\n        \n        start += chunk_size - overlap  # Move forward, keep overlap\n    \n    return chunks\n\n# Process all PDFs\nprint(\"üìñ Reading PDF files...\\n\")\nall_chunks = []\nmetadata = []\n\nfor idx, pdf_path in enumerate(PDF_PATHS):\n    print(f\"Processing: {pdf_path}\")\n    \n    if not os.path.exists(pdf_path):\n        print(f\"‚ö†Ô∏è  File not found - {pdf_path}\")\n        continue\n    \n    text = extract_text_from_pdf(pdf_path)\n    \n    if text:\n        chunks = chunk_text(text)  # Split into small pieces\n        all_chunks.extend(chunks)\n        \n        # Save info about where each chunk came from\n        for chunk_idx, chunk in enumerate(chunks):\n            metadata.append({\n                \"source\": os.path.basename(pdf_path),\n                \"chunk_id\": chunk_idx,\n                \"total_chunks\": len(chunks)\n            })\n        \n        print(f\"  ‚úÖ Created {len(chunks)} pieces\")\n    else:\n        print(f\"  ‚ö†Ô∏è  No text found\")\n\nprint(f\"\\n‚úÖ Done!\")\nprint(f\"üìä Total pieces: {len(all_chunks)}\")\nprint(f\"üìè Using chunk size: {CHUNK_SIZE} chars with {OVERLAP} char overlap\")\n\nif len(all_chunks) == 0:\n    print(\"\\n‚ö†Ô∏è  WARNING: No text found in PDFs!\")\n    print(\"Check: File paths correct? PDFs not password-protected?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Create Search Database\n",
    "\n",
    "This creates a searchable database from your PDFs.\n",
    "\n",
    "**Time:** 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Loading search model...\")\n",
    "# This model converts text to numbers for searching\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "print(\"\\nüóÑÔ∏è  Creating database...\")\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Delete old database if it exists\n",
    "try:\n",
    "    chroma_client.delete_collection(\"documents\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new database\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"documents\",\n",
    "    metadata={\"description\": f\"Documents for {PERSONA_NAME} chatbot\"}\n",
    ")\n",
    "print(\"‚úÖ Database created!\")\n",
    "\n",
    "# Add PDF pieces to database\n",
    "if len(all_chunks) > 0:\n",
    "    print(f\"\\nüì• Adding {len(all_chunks)} pieces to database...\")\n",
    "    \n",
    "    # Convert text to searchable numbers\n",
    "    embeddings = embedding_model.encode(all_chunks, show_progress_bar=True)\n",
    "    \n",
    "    # Store in database\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=all_chunks,\n",
    "        metadatas=metadata,\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(all_chunks))]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Database ready!\")\n",
    "    print(f\"üìä Total pieces in database: {collection.count()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No pieces to add!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Setup Question Answering\n",
    "\n",
    "This prepares the chatbot to answer your questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Store which PDFs were used for the last answer\nlast_sources_used = []\n\ndef retrieve_relevant_context(query, n_results=NUM_RETRIEVED_DOCS):\n    \"\"\"Find relevant pieces from your PDFs based on the question.\"\"\"\n    global last_sources_used\n    try:\n        # Convert question to searchable numbers\n        query_embedding = embedding_model.encode([query])\n        \n        # Search database for matching pieces\n        results = collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=min(n_results, collection.count())\n        )\n        \n        documents = results['documents'][0] if results['documents'] else []\n        metadatas = results['metadatas'][0] if results['metadatas'] else []\n        \n        # Track which PDFs were used\n        last_sources_used = []\n        if metadatas:\n            seen_sources = set()\n            for meta in metadatas[:2]:  # Only track the 2 pieces we actually use\n                source_name = meta.get('source', 'Unknown')\n                if source_name not in seen_sources:\n                    last_sources_used.append(source_name)\n                    seen_sources.add(source_name)\n        \n        return documents\n    except Exception as e:\n        print(f\"Error searching: {str(e)}\")\n        last_sources_used = []\n        return []\n\ndef generate_response_sync(question):\n    \"\"\"Get answer from AI using relevant PDF pieces.\"\"\"\n    # Find relevant pieces from PDFs\n    context_docs = retrieve_relevant_context(question)\n    \n    # Use only top 2 pieces, max 1500 characters (faster responses)\n    if context_docs:\n        context_docs = context_docs[:2]\n        context = \"\\n\\n\".join(context_docs)\n        context = context[:1500]\n    else:\n        context = \"No relevant documents found.\"\n    \n    # Create prompt for AI\n    prompt = f\"\"\"{PERSONA_DESCRIPTION}\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer in your persona's style:\"\"\"\n    \n    # Get answer from AI (Gemini)\n    response = model.generate_content(\n        prompt,\n        generation_config=genai.types.GenerationConfig(\n            temperature=TEMPERATURE,\n            max_output_tokens=200,  # Keep answers short\n            top_p=0.95,\n            top_k=40,\n        ),\n    )\n    \n    return response.text\n\nasync def generate_response_async(question, chat_history=None, timeout_seconds=30):\n    \"\"\"Wrapper with 30-second timeout to prevent hanging.\"\"\"\n    try:\n        # Run AI call with timeout\n        response_text = await asyncio.wait_for(\n            asyncio.to_thread(generate_response_sync, question),\n            timeout=timeout_seconds\n        )\n        return response_text\n    \n    except asyncio.TimeoutError:\n        return \"‚è±Ô∏è **Timeout** - Took too long (>30 seconds). Try a simpler question.\"\n    \n    except Exception as e:\n        error_str = str(e).lower()\n        \n        if \"429\" in str(e) or \"quota\" in error_str or \"rate limit\" in error_str:\n            return \"‚ö†Ô∏è **Rate Limit** - Wait 1-2 minutes and try again.\"\n        elif \"timeout\" in error_str or \"connection\" in error_str:\n            return \"‚ö†Ô∏è **Connection Error** - Check your internet.\"\n        elif \"blocked\" in error_str or \"safety\" in error_str:\n            return \"‚ö†Ô∏è **Content Blocked** - Try different wording.\"\n        elif \"api\" in error_str or \"key\" in error_str:\n            return \"‚ö†Ô∏è **API Error** - Check your API key.\"\n        else:\n            return f\"‚ùå **Error** - {str(e)[:100]}\"\n\nprint(\"‚úÖ Answer system ready!\")\nprint(\"‚è±Ô∏è  Response time: 5-15 seconds\")\nif SHOW_SOURCES:\n    print(\"üìö Source citations enabled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8B: Initialize Empathy Analyzer\n",
    "\n",
    "This sets up the empathy tracking system that will analyze your messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMPATHY ANALYZER - Tracks 5 dimensions of empathic communication\n",
    "# ============================================================================\n",
    "\n",
    "class EmpathyAnalyzer:\n",
    "    \"\"\"Analyzes user messages for empathy across 5 dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        self.user_messages = []\n",
    "        self.empathy_scores = []\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Empathy linguistic markers\n",
    "        self.open_question_words = ['how', 'what', 'why', 'tell', 'describe', 'explain']\n",
    "        self.emotion_words = [\n",
    "            'feel', 'feeling', 'felt', 'emotion', 'happy', 'sad', 'angry', \n",
    "            'frustrated', 'worried', 'anxious', 'excited', 'disappointed',\n",
    "            'upset', 'hurt', 'joy', 'fear', 'surprise', 'disgust', 'content',\n",
    "            'grateful', 'proud', 'ashamed', 'guilty', 'nervous', 'scared'\n",
    "        ]\n",
    "        self.perspective_phrases = [\n",
    "            'you feel', 'you might', 'from your', 'in your', 'your perspective',\n",
    "            'you seem', 'you appear', 'you sound', 'for you', 'to you',\n",
    "            'you think', 'you believe', 'you experience', 'your view'\n",
    "        ]\n",
    "        self.active_listening_phrases = [\n",
    "            'tell me more', 'i understand', 'i hear', 'i see', 'help me understand',\n",
    "            'that makes sense', 'i appreciate', 'thank you for sharing',\n",
    "            'go on', 'continue', 'interesting', 'i get it', 'i follow'\n",
    "        ]\n",
    "    \n",
    "    def analyze_message(self, message):\n",
    "        \"\"\"Analyze a single message for empathy markers.\"\"\"\n",
    "        message_lower = message.lower()\n",
    "        \n",
    "        # 1. Sentiment/Warmth (0-20 points)\n",
    "        sentiment = self.vader.polarity_scores(message)\n",
    "        warmth_score = max(0, min(20, (sentiment['compound'] + 1) * 10))\n",
    "        \n",
    "        # 2. Open Questions (0-20 points)\n",
    "        open_question_count = sum(1 for word in self.open_question_words if word in message_lower)\n",
    "        has_question = '?' in message\n",
    "        open_score = min(20, open_question_count * 10) if has_question else 0\n",
    "        \n",
    "        # 3. Emotion Words (0-20 points)\n",
    "        emotion_count = sum(1 for word in self.emotion_words if word in message_lower)\n",
    "        emotion_score = min(20, emotion_count * 7)\n",
    "        \n",
    "        # 4. Perspective-Taking (0-20 points)\n",
    "        perspective_count = sum(1 for phrase in self.perspective_phrases if phrase in message_lower)\n",
    "        perspective_score = min(20, perspective_count * 10)\n",
    "        \n",
    "        # 5. Active Listening (0-20 points)\n",
    "        listening_count = sum(1 for phrase in self.active_listening_phrases if phrase in message_lower)\n",
    "        listening_score = min(20, listening_count * 7)\n",
    "        \n",
    "        # Total score\n",
    "        total_score = warmth_score + open_score + emotion_score + perspective_score + listening_score\n",
    "        \n",
    "        return {\n",
    "            'message': message,\n",
    "            'warmth': warmth_score,\n",
    "            'open_questions': open_score,\n",
    "            'emotion_words': emotion_score,\n",
    "            'perspective_taking': perspective_score,\n",
    "            'active_listening': listening_score,\n",
    "            'total': total_score,\n",
    "            'sentiment_raw': sentiment['compound']\n",
    "        }\n",
    "    \n",
    "    def add_user_message(self, message, bot_response):\n",
    "        \"\"\"Track a user message and bot response.\"\"\"\n",
    "        analysis = self.analyze_message(message)\n",
    "        self.user_messages.append(message)\n",
    "        self.empathy_scores.append(analysis)\n",
    "        self.conversation_history.append({\n",
    "            'user': message,\n",
    "            'bot': bot_response,\n",
    "            'empathy': analysis\n",
    "        })\n",
    "    \n",
    "    def get_average_scores(self):\n",
    "        \"\"\"Calculate average scores across all messages.\"\"\"\n",
    "        if not self.empathy_scores:\n",
    "            return None\n",
    "        \n",
    "        n = len(self.empathy_scores)\n",
    "        return {\n",
    "            'warmth': sum(s['warmth'] for s in self.empathy_scores) / n,\n",
    "            'open_questions': sum(s['open_questions'] for s in self.empathy_scores) / n,\n",
    "            'emotion_words': sum(s['emotion_words'] for s in self.empathy_scores) / n,\n",
    "            'perspective_taking': sum(s['perspective_taking'] for s in self.empathy_scores) / n,\n",
    "            'active_listening': sum(s['active_listening'] for s in self.empathy_scores) / n,\n",
    "            'total': sum(s['total'] for s in self.empathy_scores) / n,\n",
    "            'message_count': n\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive empathy report.\"\"\"\n",
    "        if len(self.empathy_scores) < 10:\n",
    "            return None\n",
    "        \n",
    "        avg = self.get_average_scores()\n",
    "        total_score = avg['total']\n",
    "        \n",
    "        # Interpretation\n",
    "        if total_score >= 80:\n",
    "            interpretation = \"Excellent - Consistently demonstrates empathic responses\"\n",
    "        elif total_score >= 60:\n",
    "            interpretation = \"Good - Regular empathic responses with room to grow\"\n",
    "        elif total_score >= 40:\n",
    "            interpretation = \"Moderate - Awareness of emotions but inconsistent\"\n",
    "        elif total_score >= 20:\n",
    "            interpretation = \"Developing - Beginning to recognize emotions\"\n",
    "        else:\n",
    "            interpretation = \"Needs Practice - Focus on foundational skills\"\n",
    "        \n",
    "        # Recommendations\n",
    "        recommendations = []\n",
    "        if avg['warmth'] < 15:\n",
    "            recommendations.append(\"Use warmer, more supportive language\")\n",
    "        if avg['open_questions'] < 15:\n",
    "            recommendations.append(\"Ask more open-ended questions (what/how/why)\")\n",
    "        if avg['emotion_words'] < 15:\n",
    "            recommendations.append(\"Acknowledge emotions more explicitly\")\n",
    "        if avg['perspective_taking'] < 15:\n",
    "            recommendations.append(\"Practice perspective-taking phrases\")\n",
    "        if avg['active_listening'] < 15:\n",
    "            recommendations.append(\"Show more active listening markers\")\n",
    "        \n",
    "        # Format report\n",
    "        report = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë           EMPATHY TRAINING ANALYSIS REPORT                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üìä OVERALL EMPATHY SCORE: {total_score:.1f}/100\n",
    "   {interpretation}\n",
    "\n",
    "üìà DIMENSION BREAKDOWN:\n",
    "   ‚Ä¢ Sentiment/Warmth:      {avg['warmth']:.1f}/20 {'‚úÖ' if avg['warmth'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Open Questions:        {avg['open_questions']:.1f}/20 {'‚úÖ' if avg['open_questions'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Emotion Recognition:   {avg['emotion_words']:.1f}/20 {'‚úÖ' if avg['emotion_words'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Perspective-Taking:    {avg['perspective_taking']:.1f}/20 {'‚úÖ' if avg['perspective_taking'] >= 15 else '‚ö†Ô∏è'}\n",
    "   ‚Ä¢ Active Listening:      {avg['active_listening']:.1f}/20 {'‚úÖ' if avg['active_listening'] >= 15 else '‚ö†Ô∏è'}\n",
    "\n",
    "üìâ CONVERSATION METRICS:\n",
    "   ‚Ä¢ Total Messages Analyzed: {avg['message_count']}\n",
    "   ‚Ä¢ Average Sentiment: {sum(s['sentiment_raw'] for s in self.empathy_scores) / len(self.empathy_scores):.2f} (-1 to +1)\n",
    "   ‚Ä¢ Questions Asked: {sum(1 for s in self.empathy_scores if s['open_questions'] > 0)}\n",
    "   ‚Ä¢ Emotion Words Used: {sum(1 for s in self.empathy_scores if s['emotion_words'] > 0)} messages\n",
    "\n",
    "üí° RECOMMENDATIONS FOR IMPROVEMENT:\n",
    "\"\"\"\n",
    "        if recommendations:\n",
    "            for rec in recommendations:\n",
    "                report += f\"   ‚Ä¢ {rec}\\n\"\n",
    "        else:\n",
    "            report += \"   ‚Ä¢ Great work! Keep practicing to maintain your skills\\n\"\n",
    "        \n",
    "        report += \"\\n‚úÖ Report complete - Keep practicing empathic communication!\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def export_to_csv(self):\n",
    "        \"\"\"Export conversation data to CSV format.\"\"\"\n",
    "        import csv\n",
    "        from io import StringIO\n",
    "        \n",
    "        output = StringIO()\n",
    "        writer = csv.writer(output)\n",
    "        \n",
    "        # Header\n",
    "        writer.writerow([\n",
    "            'Message #', 'User Message', 'Bot Response', \n",
    "            'Warmth', 'Open Questions', 'Emotion Words', \n",
    "            'Perspective-Taking', 'Active Listening', 'Total Score'\n",
    "        ])\n",
    "        \n",
    "        # Data\n",
    "        for i, conv in enumerate(self.conversation_history, 1):\n",
    "            emp = conv['empathy']\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                conv['user'],\n",
    "                conv['bot'],\n",
    "                f\"{emp['warmth']:.1f}\",\n",
    "                f\"{emp['open_questions']:.1f}\",\n",
    "                f\"{emp['emotion_words']:.1f}\",\n",
    "                f\"{emp['perspective_taking']:.1f}\",\n",
    "                f\"{emp['active_listening']:.1f}\",\n",
    "                f\"{emp['total']:.1f}\"\n",
    "            ])\n",
    "        \n",
    "        return output.getvalue()\n",
    "\n",
    "# Initialize global empathy analyzer\n",
    "empathy_analyzer = EmpathyAnalyzer()\n",
    "\n",
    "print(\"‚úÖ Empathy Analyzer ready!\")\n",
    "print(\"üìä Tracking 5 dimensions:\")\n",
    "print(\"   1. Sentiment/Warmth (positive emotional tone)\")\n",
    "print(\"   2. Open Questions (exploration)\")\n",
    "print(\"   3. Emotion Recognition (naming feelings)\")\n",
    "print(\"   4. Perspective-Taking (seeing their view)\")\n",
    "print(\"   5. Active Listening (engagement)\")\n",
    "print(\"\\nüìù Report will generate after 10 messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def chat_interface(message, history):\n    \"\"\"Handle chat messages with empathy tracking and source citations.\"\"\"\n    # Get answer from AI\n    response = await generate_response_async(message, history)\n    \n    # Add source citations if enabled\n    if SHOW_SOURCES and last_sources_used:\n        response += \"\\n\\n---\\n**üìö Sources:**\\n\"\n        for i, source in enumerate(last_sources_used, 1):\n            response += f\"{i}. {source}\\n\"\n    \n    # Track empathy (user message + bot response)\n    empathy_analyzer.add_user_message(message, response)\n    \n    # Check if we've reached 10 messages - generate report\n    message_count = len(empathy_analyzer.user_messages)\n    if message_count == 10:\n        report = empathy_analyzer.generate_report()\n        if report:\n            response += \"\\n\\n\" + \"=\"*60 + \"\\n\"\n            response += report\n            response += \"\\n\" + \"=\"*60\n            response += \"\\n\\nüíæ **Want to save your data?** Run the export cell below to download as CSV.\"\n    elif message_count < 10:\n        # Show progress\n        remaining = 10 - message_count\n        response += f\"\\n\\n_üìä Empathy tracking: {message_count}/10 messages ({remaining} more for report)_\"\n    \n    return response\n\n# ============================================================================\n# STARTER QUESTIONS - OPTIONAL ‚úèÔ∏è\n# ============================================================================\n# These appear as clickable examples when chat starts\n# Change these to match your PDFs and persona\n\nSTARTER_QUESTIONS = [\n    \"What are your main beliefs or values?\",\n    \"How did that experience make you feel?\",\n    \"Tell me more about your perspective on this topic.\",\n    \"You seem passionate about this - what drives that feeling?\",\n    \"From your point of view, what are your greatest achievements?\",\n]\n\n# Create chat interface\ndemo = gr.ChatInterface(\n    fn=chat_interface,\n    title=f\"ü§ñ Chat with {PERSONA_NAME} - Empathy Training\",\n    description=f\"\"\"Practice empathic conversation with {PERSONA_NAME}.\n    \n    üìä **Empathy Assessment Enabled**\n    - Your messages are analyzed for empathy markers\n    - Report generated after 10 messages\n    - Track: warmth, questions, emotions, perspective, listening\n    \n    {'üìñ Source citations enabled - see which PDFs were used' if SHOW_SOURCES else ''}\n    \n    ‚è±Ô∏è Response time: 5-15 seconds\n    \"\"\",\n    examples=STARTER_QUESTIONS,\n)\n\n# Launch chat\nprint(\"=\" * 80)\nprint(\"üöÄ LAUNCHING EMPATHY TRAINING CHAT\")\nprint(\"=\" * 80)\nprint(\"\\nüìä EMPATHY ASSESSMENT ACTIVE\")\nprint(\"   ‚Ä¢ Tracking 5 empathy dimensions\")\nprint(\"   ‚Ä¢ Report after 10 messages\")\nprint(\"   ‚Ä¢ CSV export available\\n\")\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Use the PUBLIC LINK below (not Colab interface)\\n\")\nif SHOW_SOURCES:\n    print(\"üìö Sources ON - answers show which PDFs were used\\n\")\nprint(\"üëá COPY THIS LINK AND OPEN IN NEW TAB:\\n\")\n\ndemo.launch(\n    share=True,      # Create public link\n    inline=False,    # Don't show in Colab (unstable)\n    debug=True       # Show errors\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Chat is live with empathy tracking!\")\nprint(\"=\" * 80)\nprint(\"\\nüìå STEPS:\")\nprint(\"   1. Find 'Running on public URL:' above\")\nprint(\"   2. Copy the https://xxxxx.gradio.live link\")\nprint(\"   3. Open in new browser tab\")\nprint(\"   4. Start chatting empathically!\")\nprint(\"   5. After 10 messages, view your empathy report\")\nif SHOW_SOURCES:\n    print(\"   6. Check bottom of answers for sources\")\nprint(\"\\nüí° Link expires after 72 hours of no use\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Step 9: Export Conversation Data (Optional)\n",
    "\n",
    "**Run this after completing your conversation** to download your empathy data as CSV.\n",
    "\n",
    "This will create a file with:\n",
    "- All your messages and bot responses\n",
    "- Empathy scores for each dimension\n",
    "- Total empathy score per message\n",
    "\n",
    "You can open this in Excel or Google Sheets for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export conversation data to CSV\n",
    "if len(empathy_analyzer.conversation_history) > 0:\n",
    "    print(\"üì• Exporting conversation data...\\n\")\n",
    "    \n",
    "    csv_data = empathy_analyzer.export_to_csv()\n",
    "    \n",
    "    # Save to file\n",
    "    from google.colab import files\n",
    "    import datetime\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"empathy_conversation_{timestamp}.csv\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(csv_data)\n",
    "    \n",
    "    print(f\"‚úÖ Data exported to: {filename}\")\n",
    "    print(f\"üìä Total messages: {len(empathy_analyzer.user_messages)}\")\n",
    "    \n",
    "    avg_scores = empathy_analyzer.get_average_scores()\n",
    "    if avg_scores:\n",
    "        print(f\"üìà Average empathy score: {avg_scores['total']:.1f}/100\")\n",
    "    \n",
    "    print(\"\\nüì• Downloading file...\")\n",
    "    files.download(filename)\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "    print(\"\\nüí° You can now open this CSV file in Excel or Google Sheets\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No conversation data to export yet!\")\n",
    "    print(\"üí¨ Have a conversation first, then run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## üîÑ Step 10: Start New Conversation (Optional)\n\n**Run this to practice empathy again** with a fresh conversation.\n\nThis will:\n- Reset the empathy tracker (0/10 messages)\n- Clear previous conversation history\n- Launch a new chat interface\n\n**üí° Tip:** Export your current conversation (Step 9) BEFORE running this!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# RESET & START NEW CONVERSATION\n# ============================================================================\n\nprint(\"üîÑ Resetting empathy tracker...\\n\")\n\n# Reinitialize empathy analyzer (clears all previous data)\nempathy_analyzer = EmpathyAnalyzer()\n\nprint(\"‚úÖ Empathy tracker reset!\")\nprint(\"   ‚Ä¢ Message counter: 0/10\")\nprint(\"   ‚Ä¢ Previous conversation cleared\")\nprint(\"   ‚Ä¢ Ready for fresh practice\\n\")\n\n# Relaunch chat interface\nprint(\"=\" * 80)\nprint(\"üöÄ LAUNCHING NEW EMPATHY TRAINING CHAT\")\nprint(\"=\" * 80)\nprint(\"\\nüìä EMPATHY ASSESSMENT ACTIVE\")\nprint(\"   ‚Ä¢ Tracking 5 empathy dimensions\")\nprint(\"   ‚Ä¢ Report after 10 messages\")\nprint(\"   ‚Ä¢ CSV export available\\n\")\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Use the PUBLIC LINK below (not Colab interface)\\n\")\nif SHOW_SOURCES:\n    print(\"üìö Sources ON - answers show which PDFs were used\\n\")\nprint(\"üëá COPY THIS LINK AND OPEN IN NEW TAB:\\n\")\n\ndemo.launch(\n    share=True,      # Create public link\n    inline=False,    # Don't show in Colab (unstable)\n    debug=True       # Show errors\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ New conversation started!\")\nprint(\"=\" * 80)\nprint(\"\\nüìå STEPS:\")\nprint(\"   1. Find 'Running on public URL:' above\")\nprint(\"   2. Copy the https://xxxxx.gradio.live link\")\nprint(\"   3. Open in new browser tab\")\nprint(\"   4. Start your new empathy practice!\")\nprint(\"\\nüí° Remember: Export your previous conversation first if you haven't already\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### API Key Issues:\n",
    "- **Error: \"Invalid API key\"**\n",
    "  - Get a new key from: https://aistudio.google.com/app/apikey\n",
    "  - Make sure you copied the entire key\n",
    "  - Replace `YOUR_API_KEY_HERE` in Step 4\n",
    "\n",
    "### PDF Issues:\n",
    "- **\"File not found\" errors:**\n",
    "  - Check that Google Drive is mounted (Step 3)\n",
    "  - Verify PDF file paths are correct\n",
    "  - Make sure paths start with `/content/drive/MyDrive/`\n",
    "  \n",
    "- **\"No text extracted\":**\n",
    "  - PDF might be scanned images (not searchable text)\n",
    "  - PDF might be password-protected\n",
    "  - Try opening the PDF to verify it has selectable text\n",
    "\n",
    "### Response Issues:\n",
    "- **Responses don't match persona:**\n",
    "  - Make `PERSONA_DESCRIPTION` more detailed and specific\n",
    "  - Add more example phrases/words they use\n",
    "  \n",
    "- **Responses aren't relevant:**\n",
    "  - Increase `NUM_RETRIEVED_DOCS` (try 5 or 7)\n",
    "  - Make sure PDFs contain information about the topic\n",
    "  - Ask more specific questions\n",
    "\n",
    "### Performance Issues:\n",
    "- **Colab disconnects or times out:**\n",
    "  - This is normal for free Colab after ~12 hours\n",
    "  - Save your work and restart\n",
    "  - Keep the browser tab active\n",
    "\n",
    "### Need More Help?\n",
    "- Check Google Gemini API status: https://status.cloud.google.com/\n",
    "- Verify free tier limits haven't been exceeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Understanding the Technology\n",
    "\n",
    "### What is RAG (Retrieval-Augmented Generation)?\n",
    "RAG combines two technologies:\n",
    "1. **Retrieval**: Searching documents for relevant information\n",
    "2. **Generation**: Using AI to create natural responses\n",
    "\n",
    "### How This Notebook Works:\n",
    "1. **PDFs ‚Üí Text**: Extract text from your PDF files\n",
    "2. **Text ‚Üí Chunks**: Split into smaller, searchable pieces\n",
    "3. **Chunks ‚Üí Embeddings**: Convert to numerical representations\n",
    "4. **Store in Database**: Save in ChromaDB for fast searching\n",
    "5. **User Asks Question**: You type a question\n",
    "6. **Search Database**: Find most relevant chunks\n",
    "7. **AI Generates Answer**: Gemini creates response using context\n",
    "8. **Apply Persona**: Format response in persona's style\n",
    "\n",
    "### Why This Approach?\n",
    "- ‚úÖ **Accurate**: Responses based on your actual documents\n",
    "- ‚úÖ **Up-to-date**: Use any current information in PDFs\n",
    "- ‚úÖ **Customizable**: Change persona, style, and content easily\n",
    "- ‚úÖ **Free**: No paid services required\n",
    "- ‚úÖ **Educational**: Students learn modern AI techniques\n",
    "\n",
    "### Technologies Used:\n",
    "- **Google Gemini**: Free AI language model\n",
    "- **ChromaDB**: Vector database for semantic search\n",
    "- **Sentence Transformers**: Convert text to embeddings\n",
    "- **Gradio**: Create chat interface\n",
    "- **PyPDF**: Read PDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Next Steps & Future Features\n",
    "\n",
    "### Ideas for Enhancement:\n",
    "1. **Add more document types**: Support Word docs, web pages, etc.\n",
    "2. **Conversation memory**: Remember previous questions in the chat\n",
    "3. **Source citations**: Show which PDF the answer came from\n",
    "4. **Multiple personas**: Switch between different personalities\n",
    "5. **Voice input/output**: Add speech recognition and text-to-speech\n",
    "6. **Fact-checking mode**: Verify claims against documents\n",
    "7. **Export conversations**: Save chat history\n",
    "8. **Advanced search**: Filter by document, date, topic, etc.\n",
    "\n",
    "### Learning Resources:\n",
    "- Google Gemini API Docs: https://ai.google.dev/docs\n",
    "- ChromaDB Documentation: https://docs.trychroma.com/\n",
    "- Gradio Documentation: https://www.gradio.app/docs/\n",
    "- RAG Overview: https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Chatting! üéâ**\n",
    "\n",
    "*Created for educational purposes. Completely free and customizable.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}