{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot - Empathy Training Version\n",
    "\n",
    "## What This Does\n",
    "\n",
    "This chatbot answers questions about your PDF documents **AND** trains your empathy skills.\n",
    "\n",
    "**Features:**\n",
    "- Ask questions, get answers from your PDFs\n",
    "- Your questions are analyzed for empathy\n",
    "- Get empathy score after 20 messages\n",
    "\n",
    "**You Need:**\n",
    "- PDF files in your Google Drive\n",
    "- A free Google Gemini API key\n",
    "- Internet connection\n",
    "\n",
    "**Cost:** 100% FREE\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** See STUDENT_GUIDE.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "This installs all the tools we need:\n",
    "- `chromadb`: Creates a searchable database of documents\n",
    "- `gradio`: Creates the chat interface\n",
    "- `pypdf`: Reads PDF files\n",
    "- `sentence-transformers`: Converts text to searchable format\n",
    "- `google-generativeai`: Connects to Google's free AI (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages (including empathy analysis)\n",
    "!pip install -q chromadb gradio pypdf sentence-transformers google-generativeai vaderSentiment\n",
    "\n",
    "print(\"âœ… All libraries installed successfully!\")\n",
    "print(\"ğŸ“Š Empathy analysis module included!\")\n",
    "print(\"\\nâ„¹ï¸  Note: You may see dependency warnings about 'opentelemetry' packages.\")\n",
    "print(\"   These are non-critical and won't affect functionality. You can safely ignore them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Import Libraries\n",
    "\n",
    "Load all the tools we just installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from google.colab import drive\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Mount Google Drive\n",
    "\n",
    "This connects to your Google Drive so we can access your PDF files.\n",
    "\n",
    "**You'll need to:**\n",
    "1. Click the link that appears\n",
    "2. Choose your Google account\n",
    "3. Allow access\n",
    "4. Your Drive will appear in `/content/drive/MyDrive/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… Google Drive mounted successfully!\")\n",
    "print(\"ğŸ“ Your files are available at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Settings - CHANGE THESE! âœï¸\n",
    "\n",
    "**What to do:**\n",
    "1. Get a FREE API key: https://aistudio.google.com/app/apikey\n",
    "2. Paste it in Cell 8 below\n",
    "3. Update your PDF paths\n",
    "4. (Optional) Change the persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# 1. API KEY - CHANGE THIS! âœï¸\n# ============================================================================\n# Get your free key: https://aistudio.google.com/app/apikey\nGEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # â† PASTE YOUR KEY HERE\n\n# ============================================================================\n# 2. PERSONA - CUSTOMIZE THIS! âœï¸\n# ============================================================================\nPERSONA_NAME = \"Your Persona Name\"  # âœï¸ CHANGE THIS - e.g., \"Albert Einstein\", \"Oprah Winfrey\"\n\n# âœï¸ CUSTOMIZE THIS: Describe your persona's speaking style and personality\nPERSONA_DESCRIPTION = \"\"\"\nReplace this entire section with your persona's description.\n\nTemplate:\nYou are [NAME], [brief description/title/role].\nYou speak in a [adjective] manner, using [characteristic words/phrases].\nYou emphasize [what they care about] and often [communication patterns].\n\nInstructions for customization:\n- Describe HOW they speak (tone, word choice, sentence structure)\n- Include specific phrases or words they commonly use\n- Mention what topics/themes they emphasize\n- Note any unique speaking patterns or habits\n- Keep it focused on communication style, not just biographical facts\n\nExample:\n\"You are Marie Curie, pioneering scientist. You speak precisely and scientifically,\nusing terms like 'research,' 'experiment,' and 'discovery.' You emphasize evidence-based\nreasoning and the importance of persistence in scientific work.\"\n\"\"\"\n\n# ============================================================================\n# 3. RESPONSE SETTINGS - OPTIONAL\n# ============================================================================\nTEMPERATURE = 0.7  # Creativity level (0.0 = focused, 1.0 = creative)\nMAX_OUTPUT_TOKENS = 500  # Maximum response length (~375 words)\nNUM_RETRIEVED_DOCS = 7  # How many document pieces to search\n\n# ============================================================================\n# 4. SOURCE CITATIONS - OPTIONAL\n# ============================================================================\nSHOW_SOURCES = True  # True = show which PDFs were used, False = hide\n\n# ============================================================================\n# 5. PDF FILES - CHANGE THIS! âœï¸\n# ============================================================================\nPDF_PATHS = [\n    \"/content/drive/MyDrive/your_folder/document1.pdf\",  # â† CHANGE THESE\n    \"/content/drive/MyDrive/your_folder/document2.pdf\",  # â† TO YOUR PATHS\n    \"/content/drive/MyDrive/your_folder/document3.pdf\",\n]\n\n# ============================================================================\n# API SETUP (Don't change this)\n# ============================================================================\ngenai.configure(api_key=GEMINI_API_KEY)  # Connect to Google Gemini\nmodel = genai.GenerativeModel('gemini-2.0-flash')  # Use fast, free model\n\nprint(\"âœ… Configuration complete!\")\nprint(f\"ğŸ“‹ Persona: {PERSONA_NAME}\")\nprint(f\"ğŸ¤– Model: gemini-2.0-flash\")\nprint(f\"ğŸ“„ Number of PDF files configured: {len(PDF_PATHS)}\")\nprint(f\"ğŸŒ¡ï¸  Temperature: {TEMPERATURE} (range: 0.0-1.0)\")\nprint(f\"ğŸ“Š Documents retrieved per query: {NUM_RETRIEVED_DOCS}\")\nprint(f\"ğŸ“š Show source citations: {'ON âœ…' if SHOW_SOURCES else 'OFF'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª API Connection Test (Run this first!)\n",
    "\n",
    "**IMPORTANT: Run this cell to verify your API key works BEFORE proceeding!**\n",
    "\n",
    "This simple test will:\n",
    "1. Make a quick API call to Gemini\n",
    "2. Verify your API key is working\n",
    "3. Show you that the API is responding\n",
    "\n",
    "If this works, we know the API is fine. If it hangs, we know it's an API/network issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ§ª Testing API connection...\")\nprint(\"=\" * 60)\n\ntry:\n    # Simple test prompt\n    test_response = model.generate_content(\n        \"Say 'Hello! API is working!' in a friendly, enthusiastic style.\",\n        generation_config=genai.types.GenerationConfig(\n            temperature=0.7,\n            max_output_tokens=50,\n        ),\n    )\n    \n    print(\"âœ… SUCCESS! API is working!\")\n    print(f\"\\nTest Response: {test_response.text}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"âœ… You can proceed with the rest of the notebook!\")\n    \nexcept Exception as e:\n    print(f\"âŒ API TEST FAILED!\")\n    print(f\"Error: {str(e)}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"âš ï¸  STOP! Fix this issue before proceeding:\")\n    print(\"  1. Check your API key is correct\")\n    print(\"  2. Check your internet connection\")\n    print(\"  3. Visit https://aistudio.google.com/app/apikey to verify your key\")\n    print(\"  4. Check API status at https://status.cloud.google.com/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load and Process PDF Files\n",
    "\n",
    "This step:\n",
    "1. Reads all your PDF files\n",
    "2. Extracts the text\n",
    "3. Splits the text into smaller chunks (for better searching)\n",
    "\n",
    "**This may take a minute depending on the size of your PDFs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into small pieces (chunks) for better searching.\n",
    "    \n",
    "    chunk_size=1000: Each piece is 1000 characters (~150 words)\n",
    "    overlap=200: Pieces overlap by 200 characters to keep context\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Only add non-empty chunks\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start += chunk_size - overlap  # Move forward, but keep some overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process all PDFs\n",
    "print(\"ğŸ“– Processing PDF files...\\n\")\n",
    "all_chunks = []\n",
    "metadata = []\n",
    "\n",
    "for idx, pdf_path in enumerate(PDF_PATHS):\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"âš ï¸  Warning: File not found - {pdf_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    if text:\n",
    "        # Split into chunks\n",
    "        chunks = chunk_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        # Store metadata for each chunk (tracks which file it came from)\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            metadata.append({\n",
    "                \"source\": os.path.basename(pdf_path),\n",
    "                \"chunk_id\": chunk_idx,\n",
    "                \"total_chunks\": len(chunks)\n",
    "            })\n",
    "        \n",
    "        print(f\"  âœ… Extracted {len(chunks)} chunks\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  No text extracted\")\n",
    "\n",
    "print(f\"\\nâœ… Processing complete!\")\n",
    "print(f\"ğŸ“Š Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "if len(all_chunks) == 0:\n",
    "    print(\"\\nâš ï¸  WARNING: No text was extracted from PDFs!\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"  1. PDF file paths are correct\")\n",
    "    print(\"  2. Files exist in your Google Drive\")\n",
    "    print(\"  3. PDFs are not password-protected or image-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Create Vector Database (ChromaDB)\n",
    "\n",
    "This step:\n",
    "1. Converts all text chunks into \"embeddings\" (numerical representations)\n",
    "2. Stores them in ChromaDB for fast searching\n",
    "3. Allows us to find relevant context when a question is asked\n",
    "\n",
    "**This may take 1-2 minutes for large document sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ Initializing embedding model...\")\n",
    "# Load a free search model that converts text to numbers for searching\n",
    "# Model: 'all-MiniLM-L6-v2' - fast, accurate, and free\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Embedding model loaded!\")\n",
    "\n",
    "print(\"\\nğŸ—„ï¸  Creating ChromaDB database...\")\n",
    "# Initialize ChromaDB (our vector database for searching)\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,  # Don't send usage data\n",
    "    allow_reset=True  # Allow database reset if needed\n",
    "))\n",
    "\n",
    "# Create or get collection (a collection is like a folder for documents)\n",
    "try:\n",
    "    chroma_client.delete_collection(\"documents\")  # Delete old collection if exists\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"documents\",\n",
    "    metadata={\"description\": f\"Document collection for {PERSONA_NAME} chatbot\"}\n",
    ")\n",
    "\n",
    "print(\"âœ… Database created!\")\n",
    "\n",
    "# Add documents to the database\n",
    "if len(all_chunks) > 0:\n",
    "    print(f\"\\nğŸ“¥ Adding {len(all_chunks)} chunks to database...\")\n",
    "    print(\"â³ This may take a moment...\")\n",
    "    \n",
    "    # Convert text chunks to numbers (embeddings) for searching\n",
    "    embeddings = embedding_model.encode(all_chunks, show_progress_bar=True)\n",
    "    \n",
    "    # Store in ChromaDB with metadata (source file, chunk number)\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=all_chunks,\n",
    "        metadatas=metadata,\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(all_chunks))]\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… All chunks added to database!\")\n",
    "    print(f\"ğŸ“Š Database contains {collection.count()} document chunks\")\n",
    "else:\n",
    "    print(\"âš ï¸  No chunks to add to database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Create RAG Query Function\n",
    "\n",
    "This is the \"brain\" of the chatbot. It:\n",
    "1. Takes a user's question\n",
    "2. Searches the database for relevant information\n",
    "3. Sends the question + relevant context to Google Gemini\n",
    "4. Returns a response in the persona's style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Empathy Analysis Module ğŸ“Š\n",
    "\n",
    "**This is the empathy tracking system!**\n",
    "\n",
    "This module analyzes YOUR messages to measure empathy across 5 dimensions:\n",
    "1. Sentiment/warmth\n",
    "2. Open question ratio  \n",
    "3. Emotion word usage\n",
    "4. Perspective-taking\n",
    "5. Active listening\n",
    "\n",
    "After 20 messages, you'll receive a comprehensive empathy report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import statistics\n",
    "import datetime\n",
    "\n",
    "class EmpathyAnalyzer:\n",
    "    \"\"\"Analyzes student messages for empathy indicators.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()  # Tool to measure positive/negative tone\n",
    "        self.conversation_data = []  # Stores all analyzed messages\n",
    "        \n",
    "    def analyze_message(self, message):\n",
    "        \"\"\"Analyze single student message for empathy markers.\"\"\"\n",
    "        text = message.lower()\n",
    "        \n",
    "        # Store analysis results for this message\n",
    "        analysis = {\n",
    "            'message': message,\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'sentiment': self.vader.polarity_scores(text)['compound'],  # -1 (negative) to +1 (positive)\n",
    "            'open_questions': self._count_open_questions(text),  # How many \"how/why\" questions\n",
    "            'emotion_words': self._count_emotion_words(text),  # Words like \"feel\", \"sad\", \"happy\"\n",
    "            'perspective_taking': self._count_perspective_taking(text),  # Phrases like \"you feel\", \"you think\"\n",
    "            'active_listening': self._count_active_listening(text),  # Phrases like \"tell me more\"\n",
    "            'word_count': len(message.split())\n",
    "        }\n",
    "        \n",
    "        self.conversation_data.append(analysis)\n",
    "        return analysis\n",
    "    \n",
    "    def _count_open_questions(self, text):\n",
    "        \"\"\"Count open-ended question words (what, how, why, etc.)\"\"\"\n",
    "        open_markers = ['what', 'how', 'why', 'tell me', 'describe', 'explain', 'could you']\n",
    "        return sum(1 for marker in open_markers if marker in text)\n",
    "    \n",
    "    def _count_emotion_words(self, text):\n",
    "        \"\"\"Count emotion-related words\"\"\"\n",
    "        emotion_dict = ['feel', 'felt', 'emotion', 'happy', 'sad', 'angry', 'afraid', \n",
    "                       'worried', 'excited', 'frustrated', 'anxious', 'upset', 'joy', \n",
    "                       'nervous', 'scared', 'content', 'pleased', 'disappointed']\n",
    "        return sum(text.count(word) for word in emotion_dict)\n",
    "    \n",
    "    def _count_perspective_taking(self, text):\n",
    "        \"\"\"Count phrases that show understanding of other person's viewpoint\"\"\"\n",
    "        perspective_phrases = ['you feel', 'you think', 'you must', 'your perspective',\n",
    "                              'in your situation', 'you seem', 'you might', 'from your view',\n",
    "                              'in your shoes', 'you believe', 'you experience']\n",
    "        return sum(1 for phrase in perspective_phrases if phrase in text)\n",
    "    \n",
    "    def _count_active_listening(self, text):\n",
    "        \"\"\"Count phrases that show you're listening and engaged\"\"\"\n",
    "        listening_markers = ['tell me more', 'i understand', 'i see', 'that makes sense',\n",
    "                            'help me understand', 'i hear you', 'go on', 'continue',\n",
    "                            'interesting', 'i appreciate']\n",
    "        return sum(1 for marker in listening_markers if marker in text)\n",
    "    \n",
    "    def get_empathy_score(self):\n",
    "        \"\"\"Calculate overall empathy score (0-100).\"\"\"\n",
    "        if not self.conversation_data:\n",
    "            return 0\n",
    "        \n",
    "        # Component scores (how many markers were found)\n",
    "        avg_sentiment = statistics.mean([d['sentiment'] for d in self.conversation_data])\n",
    "        total_open_q = sum([d['open_questions'] for d in self.conversation_data])\n",
    "        total_emotion = sum([d['emotion_words'] for d in self.conversation_data])\n",
    "        total_perspective = sum([d['perspective_taking'] for d in self.conversation_data])\n",
    "        total_listening = sum([d['active_listening'] for d in self.conversation_data])\n",
    "        \n",
    "        num_messages = len(self.conversation_data)\n",
    "        \n",
    "        # Normalize scores (0-20 points each dimension, total = 100)\n",
    "        sentiment_score = (avg_sentiment + 1) * 10  # -1 to +1 â†’ 0 to 20\n",
    "        question_score = min(total_open_q / num_messages * 20, 20)\n",
    "        emotion_score = min(total_emotion / num_messages * 10, 20)\n",
    "        perspective_score = min(total_perspective / num_messages * 20, 20)\n",
    "        listening_score = min(total_listening / num_messages * 20, 20)\n",
    "        \n",
    "        total = sentiment_score + question_score + emotion_score + perspective_score + listening_score\n",
    "        \n",
    "        return total  # Score out of 100\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Generate detailed empathy report with interpretation and recommendations.\"\"\"\n",
    "        score = self.get_empathy_score()\n",
    "        \n",
    "        # Interpret the score\n",
    "        interpretation = \"\"\n",
    "        if score >= 80:\n",
    "            interpretation = \"Excellent - Strong empathy demonstrated consistently\"\n",
    "        elif score >= 60:\n",
    "            interpretation = \"Good - Regular empathic responses with room to grow\"\n",
    "        elif score >= 40:\n",
    "            interpretation = \"Moderate - Shows awareness but inconsistent\"\n",
    "        elif score >= 20:\n",
    "            interpretation = \"Developing - Building empathy skills\"\n",
    "        else:\n",
    "            interpretation = \"Needs Practice - Focus on active listening\"\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_sentiment = statistics.mean([d['sentiment'] for d in self.conversation_data]) if self.conversation_data else 0\n",
    "        total_open_q = sum([d['open_questions'] for d in self.conversation_data])\n",
    "        total_emotion = sum([d['emotion_words'] for d in self.conversation_data])\n",
    "        total_perspective = sum([d['perspective_taking'] for d in self.conversation_data])\n",
    "        total_listening = sum([d['active_listening'] for d in self.conversation_data])\n",
    "        \n",
    "        # Generate report\n",
    "        report = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           EMPATHY TRAINING ANALYSIS REPORT                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“Š OVERALL EMPATHY SCORE: {score:.1f}/100\n",
    "   {interpretation}\n",
    "\n",
    "ğŸ“ˆ CONVERSATION METRICS:\n",
    "   â€¢ Total Messages: {len(self.conversation_data)}\n",
    "   â€¢ Average Sentiment: {avg_sentiment:.2f} (-1 to +1)\n",
    "   â€¢ Total Open Questions: {total_open_q}\n",
    "   â€¢ Total Emotion Words: {total_emotion}\n",
    "   â€¢ Perspective-Taking Attempts: {total_perspective}\n",
    "   â€¢ Active Listening Markers: {total_listening}\n",
    "\n",
    "ğŸ’¡ RECOMMENDATIONS:\n",
    "\"\"\"\n",
    "        # Add specific suggestions based on weaknesses\n",
    "        if total_open_q < len(self.conversation_data):\n",
    "            report += \"   â€¢ Try asking more open-ended questions (what/how/why)\\n\"\n",
    "        if total_emotion < len(self.conversation_data) * 2:\n",
    "            report += \"   â€¢ Acknowledge emotions more explicitly (\\\"you seem...\\\", \\\"you feel...\\\")\\n\"\n",
    "        if total_perspective < len(self.conversation_data):\n",
    "            report += \"   â€¢ Practice perspective-taking (\\\"from your view...\\\", \\\"in your situation...\\\")\\n\"\n",
    "        if avg_sentiment < 0.2:\n",
    "            report += \"   â€¢ Use warmer, more supportive language\\n\"\n",
    "        if total_listening < len(self.conversation_data):\n",
    "            report += \"   â€¢ Show more active listening (\\\"tell me more\\\", \\\"I understand\\\")\\n\"\n",
    "        \n",
    "        if score >= 80:\n",
    "            report += \"   â€¢ Excellent work! You're demonstrating strong empathy consistently.\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize analyzer\n",
    "empathy_analyzer = EmpathyAnalyzer()\n",
    "\n",
    "print(\"âœ… Empathy Analysis Module loaded!\")\n",
    "print(\"ğŸ“Š Student responses will be analyzed for empathy indicators\")\n",
    "print(\"â±ï¸  After 20 messages, you'll receive a comprehensive empathy report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Global variable to store which PDF files were used for the last answer\nlast_sources_used = []\n\ndef retrieve_relevant_context(query, n_results=NUM_RETRIEVED_DOCS):\n    \"\"\"Search the database for relevant document chunks and return with metadata.\"\"\"\n    global last_sources_used\n    try:\n        # Convert question to numbers (embedding) for searching\n        query_embedding = embedding_model.encode([query])\n        \n        # Search ChromaDB for similar chunks\n        results = collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=min(n_results, collection.count())  # Get up to N chunks\n        )\n        \n        # Extract documents and metadata\n        documents = results['documents'][0] if results['documents'] else []\n        metadatas = results['metadatas'][0] if results['metadatas'] else []\n        \n        # Track which PDF files were used (for source citations)\n        last_sources_used = []\n        if metadatas:\n            seen_sources = set()  # Avoid duplicates\n            for meta in metadatas[:2]:  # Only track the 2 chunks we actually use\n                source_name = meta.get('source', 'Unknown')\n                if source_name not in seen_sources:\n                    last_sources_used.append(source_name)\n                    seen_sources.add(source_name)\n        \n        return documents\n    except Exception as e:\n        print(f\"Error in retrieval: {str(e)}\")\n        last_sources_used = []\n        return []\n\ndef generate_response_sync(question):\n    \"\"\"Call Gemini API to generate an answer (synchronous version).\n    \n    This runs in a separate thread with timeout control.\n    \"\"\"\n    # Step 1: Find relevant chunks from PDFs\n    context_docs = retrieve_relevant_context(question)\n    \n    # Step 2: Reduce context size for faster processing\n    if context_docs:\n        context_docs = context_docs[:2]  # Use only 2 chunks (not all 7)\n        context = \"\\n\\n\".join(context_docs)\n        context = context[:1500]  # Limit to 1500 characters\n    else:\n        context = \"No relevant documents found.\"\n    \n    # Step 3: Create prompt with context + persona + question\n    prompt = f\"\"\"{PERSONA_DESCRIPTION}\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer in your persona's style:\"\"\"\n    \n    # Step 4: Call Gemini API\n    response = model.generate_content(\n        prompt,\n        generation_config=genai.types.GenerationConfig(\n            temperature=TEMPERATURE,  # Creativity level\n            max_output_tokens=200,  # Response length (reduced for speed)\n            top_p=0.95,  # Sampling diversity\n            top_k=40,  # Sampling diversity\n        ),\n    )\n    \n    return response.text\n\nasync def generate_response_async(question, chat_history=None, timeout_seconds=30):\n    \"\"\"Async wrapper with 30-second timeout to prevent hanging.\n    \n    Args:\n        question: The user's question\n        chat_history: Previous conversation (optional)\n        timeout_seconds: Maximum wait time (default: 30 seconds)\n    \n    Returns:\n        The chatbot's response or error message\n    \"\"\"\n    try:\n        # Run the synchronous function in a separate thread with timeout\n        response_text = await asyncio.wait_for(\n            asyncio.to_thread(generate_response_sync, question),\n            timeout=timeout_seconds  # Hard timeout after 30 seconds\n        )\n        \n        return response_text\n    \n    except asyncio.TimeoutError:\n        # API took too long\n        return \"â±ï¸ **Timeout Error**\\n\\nThe API took too long (>30 seconds).\\n\\n**Try:**\\n1. Ask a shorter, simpler question\\n2. Wait 30 seconds and try again\\n3. Check your internet connection\"\n    \n    except Exception as e:\n        error_str = str(e).lower()\n        \n        # Rate limit errors (too many requests)\n        if \"429\" in str(e) or \"quota\" in error_str or \"rate limit\" in error_str or \"resource exhausted\" in error_str:\n            return \"âš ï¸ **Rate Limit**\\n\\nThe free API has request limits. Please wait 1-2 minutes and try again.\"\n        \n        # Connection/timeout errors\n        elif \"timeout\" in error_str or \"connection\" in error_str or \"deadline\" in error_str:\n            return \"âš ï¸ **Connection Error**\\n\\nAPI connection failed.\\n\\n**Try:**\\n1. Check your internet connection\\n2. Wait a moment and try again\\n3. Check API status at https://status.cloud.google.com/\"\n        \n        # Content safety filter\n        elif \"blocked\" in error_str or \"safety\" in error_str:\n            return \"âš ï¸ **Content Blocked**\\n\\nResponse blocked by safety filters. Try rephrasing your question.\"\n        \n        # API key/auth errors\n        elif \"api\" in error_str or \"invalid\" in error_str or \"auth\" in error_str or \"key\" in error_str:\n            return f\"âš ï¸ **API Error**\\n\\n{str(e)}\\n\\n**Check:**\\n1. Your API key is valid\\n2. You have internet connectivity\"\n        \n        # Other errors\n        else:\n            return f\"âŒ **Unexpected Error**\\n\\n{str(e)[:200]}\\n\\n**Try:**\\n1. Restarting the kernel (Runtime â†’ Restart runtime)\\n2. Re-running all cells\"\n\nprint(\"âœ… Async RAG function created with 30-second timeout!\")\nprint(\"ğŸ¤– The chatbot will respond within 30 seconds or show a timeout message.\")\nprint(\"â±ï¸  Expected response time: 5-15 seconds\")\nif SHOW_SOURCES:\n    print(\"ğŸ“š Source citations enabled - you'll see which PDFs were used!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def chat_interface(message, history):\n    \"\"\"Chat handler with empathy tracking.\n    \n    Args:\n        message: Current user message\n        history: List of previous messages\n    \n    Returns:\n        Bot's response (includes empathy report after 20 messages)\n    \"\"\"\n    # Analyze student's message for empathy\n    empathy_analysis = empathy_analyzer.analyze_message(message)\n    \n    # Generate bot response\n    response = await generate_response_async(message, history)\n    \n    # Append source citations if enabled (before empathy progress)\n    if SHOW_SOURCES and last_sources_used:\n        response += \"\\n\\n---\\n**ğŸ“š Sources used:**\\n\"\n        for i, source in enumerate(last_sources_used, 1):\n            response += f\"{i}. {source}\\n\"\n        response += \"\\nğŸ’¡ *Set `SHOW_SOURCES = False` in Cell 8 to hide citations*\"\n    \n    # Check if we've reached 20 messages\n    message_count = len(empathy_analyzer.conversation_data)\n    \n    if message_count == 20:\n        # Generate and append empathy report\n        report = empathy_analyzer.get_report()\n        response += f\"\\n\\n{'='*60}\\n{report}\\n{'='*60}\"\n        response += \"\\n\\nğŸ‰ **Empathy training session complete!** You can continue chatting if you'd like.\"\n    elif message_count < 20:\n        # Show progress\n        remaining = 20 - message_count\n        response += f\"\\n\\nğŸ“Š *Message {message_count}/20 â€¢ {remaining} messages until empathy report*\"\n    \n    return response\n\n# Create Gradio ChatInterface with empathy tracking\ndemo = gr.ChatInterface(\n    fn=chat_interface,  # Function to handle chat\n    title=f\"ğŸ¤– Chat with {PERSONA_NAME} - Empathy Training Edition\",\n    description=f\"\"\"Ask questions and get answers in {PERSONA_NAME}'s distinctive style!\n    \n    ğŸ“š **Documents:** Your uploaded PDF files will be used to provide factual answers.\n    \n    ğŸ“Š **Empathy Tracking:** Your messages are analyzed for empathy. After 20 messages, you'll receive a detailed report with your score and improvement tips.\n    \n    ğŸ’¡ **Tips:** \n    - Ask questions related to your PDF documents\n    - Demonstrate empathy: use \"you feel\", ask \"how/why\", acknowledge emotions\n    {'- Source citations are ON - see which documents were used!' if SHOW_SOURCES else ''}\n    \n    â±ï¸ **Response time:** 5-15 seconds (30 second max)\n    \"\"\",\n    examples=[\n        \"What are your main beliefs or values?\",\n        \"How did that experience make you feel?\",\n        \"Tell me more about your perspective on this topic.\",\n        \"You seem passionate about this - what drives that feeling?\",\n        \"From your point of view, what are your greatest achievements?\",\n    ],\n)\n\n# Launch with stable external link\nprint(\"=\" * 80)\nprint(\"ğŸš€ LAUNCHING EMPATHY TRAINING CHATBOT\")\nprint(\"=\" * 80)\nprint(\"\\nâš ï¸  IMPORTANT: Use the PUBLIC LINK below (not the Colab interface)\\n\")\nprint(\"The public link is stable. The Colab interface may restart.\\n\")\nif SHOW_SOURCES:\n    print(\"ğŸ“š Source citations are ON (change SHOW_SOURCES in Cell 8 to disable)\\n\")\nprint(\"ğŸ‘‡ COPY AND OPEN THIS LINK:\\n\")\n\ndemo.launch(\n    share=True,      # Create public link\n    inline=False,    # Don't use Colab interface (unstable)\n    debug=True       # Show errors\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"âœ… Chatbot is live with empathy tracking!\")\nprint(\"=\" * 80)\nprint(\"\\nğŸ“‹ INSTRUCTIONS:\")\nprint(\"   1. Find the 'Running on public URL:' message above\")\nprint(\"   2. Copy the https://xxxxx.gradio.live URL\")\nprint(\"   3. Open it in a new browser tab\")\nprint(\"   4. Start chatting!\")\nprint(\"   5. After 20 messages, you'll get your empathy report\")\nif SHOW_SOURCES:\n    print(\"   6. Check the bottom of each answer for source citations\")\nprint(\"\\nğŸ’¡ Link expires after 72 hours of inactivity.\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Export Conversation Data (Optional)\n",
    "\n",
    "After completing your conversation, you can export your empathy data to CSV for further analysis or to track your progress over time.\n",
    "\n",
    "**To export:**\n",
    "1. Uncomment the last line in the cell below\n",
    "2. Run the cell\n",
    "3. Find the file in your Google Drive root folder\n",
    "4. Download it for offline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def export_conversation_to_csv(filename='empathy_conversation.csv'):\n",
    "    \"\"\"Export conversation data with empathy metrics to CSV.\"\"\"\n",
    "    \n",
    "    if len(empathy_analyzer.conversation_data) == 0:\n",
    "        print(\"âŒ No conversation data to export yet!\")\n",
    "        print(\"ğŸ’¬ Start chatting first, then come back to export.\")\n",
    "        return\n",
    "    \n",
    "    # Create filepath in Google Drive\n",
    "    filepath = f\"/content/drive/MyDrive/{filename}\"\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['timestamp', 'message', 'word_count', \n",
    "                                                'sentiment', 'open_questions', 'emotion_words',\n",
    "                                                'perspective_taking', 'active_listening'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(empathy_analyzer.conversation_data)\n",
    "    \n",
    "    # Calculate and print summary\n",
    "    score = empathy_analyzer.get_empathy_score()\n",
    "    \n",
    "    print(f\"âœ… Conversation exported successfully!\")\n",
    "    print(f\"ğŸ“ File location: {filepath}\")\n",
    "    print(f\"ğŸ“Š Total messages: {len(empathy_analyzer.conversation_data)}\")\n",
    "    print(f\"ğŸ¯ Your empathy score: {score:.1f}/100\")\n",
    "    print(\"\\nğŸ’¡ You can download this file from Google Drive to analyze further!\")\n",
    "\n",
    "# Uncomment the line below to export your conversation:\n",
    "# export_conversation_to_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”§ Troubleshooting\n",
    "\n",
    "### API Key Issues:\n",
    "- **Error: \"Invalid API key\"**\n",
    "  - Get a new key from: https://aistudio.google.com/app/apikey\n",
    "  - Make sure you copied the entire key\n",
    "  - Replace `YOUR_API_KEY_HERE` in Step 4\n",
    "\n",
    "### PDF Issues:\n",
    "- **\"File not found\" errors:**\n",
    "  - Check that Google Drive is mounted (Step 3)\n",
    "  - Verify PDF file paths are correct\n",
    "  - Make sure paths start with `/content/drive/MyDrive/`\n",
    "  \n",
    "- **\"No text extracted\":**\n",
    "  - PDF might be scanned images (not searchable text)\n",
    "  - PDF might be password-protected\n",
    "  - Try opening the PDF to verify it has selectable text\n",
    "\n",
    "### Response Issues:\n",
    "- **Responses don't match persona:**\n",
    "  - Make `PERSONA_DESCRIPTION` more detailed and specific\n",
    "  - Add more example phrases/words they use\n",
    "  \n",
    "- **Responses aren't relevant:**\n",
    "  - Increase `NUM_RETRIEVED_DOCS` (try 5 or 7)\n",
    "  - Make sure PDFs contain information about the topic\n",
    "  - Ask more specific questions\n",
    "\n",
    "### Performance Issues:\n",
    "- **Colab disconnects or times out:**\n",
    "  - This is normal for free Colab after ~12 hours\n",
    "  - Save your work and restart\n",
    "  - Keep the browser tab active\n",
    "\n",
    "### Need More Help?\n",
    "- Check Google Gemini API status: https://status.cloud.google.com/\n",
    "- Verify free tier limits haven't been exceeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ Understanding the Technology\n",
    "\n",
    "### What is RAG (Retrieval-Augmented Generation)?\n",
    "RAG combines two technologies:\n",
    "1. **Retrieval**: Searching documents for relevant information\n",
    "2. **Generation**: Using AI to create natural responses\n",
    "\n",
    "### How This Notebook Works:\n",
    "1. **PDFs â†’ Text**: Extract text from your PDF files\n",
    "2. **Text â†’ Chunks**: Split into smaller, searchable pieces\n",
    "3. **Chunks â†’ Embeddings**: Convert to numerical representations\n",
    "4. **Store in Database**: Save in ChromaDB for fast searching\n",
    "5. **User Asks Question**: You type a question\n",
    "6. **Search Database**: Find most relevant chunks\n",
    "7. **AI Generates Answer**: Gemini creates response using context\n",
    "8. **Apply Persona**: Format response in persona's style\n",
    "\n",
    "### Why This Approach?\n",
    "- âœ… **Accurate**: Responses based on your actual documents\n",
    "- âœ… **Up-to-date**: Use any current information in PDFs\n",
    "- âœ… **Customizable**: Change persona, style, and content easily\n",
    "- âœ… **Free**: No paid services required\n",
    "- âœ… **Educational**: Students learn modern AI techniques\n",
    "\n",
    "### Technologies Used:\n",
    "- **Google Gemini**: Free AI language model\n",
    "- **ChromaDB**: Vector database for semantic search\n",
    "- **Sentence Transformers**: Convert text to embeddings\n",
    "- **Gradio**: Create chat interface\n",
    "- **PyPDF**: Read PDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ Next Steps & Future Features\n",
    "\n",
    "### Ideas for Enhancement:\n",
    "1. **Add more document types**: Support Word docs, web pages, etc.\n",
    "2. **Conversation memory**: Remember previous questions in the chat\n",
    "3. **Source citations**: Show which PDF the answer came from\n",
    "4. **Multiple personas**: Switch between different personalities\n",
    "5. **Voice input/output**: Add speech recognition and text-to-speech\n",
    "6. **Fact-checking mode**: Verify claims against documents\n",
    "7. **Export conversations**: Save chat history\n",
    "8. **Advanced search**: Filter by document, date, topic, etc.\n",
    "\n",
    "### Learning Resources:\n",
    "- Google Gemini API Docs: https://ai.google.dev/docs\n",
    "- ChromaDB Documentation: https://docs.trychroma.com/\n",
    "- Gradio Documentation: https://www.gradio.app/docs/\n",
    "- RAG Overview: https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Chatting! ğŸ‰**\n",
    "\n",
    "*Created for educational purposes. Completely free and customizable.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}